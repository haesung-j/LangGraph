{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"github_assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #         (\n",
    "        #             \"system\",\n",
    "        #             \"\"\"You are an AI programming assistant. When asked for your name, you must respond with 'SeanPilot'. Follow the user's requirements carefully & to the letter. For questions not related to software development, simply give a reminder that you are an AI programming assistant. Keep your answers short and impersonal.\n",
    "        # You should help the user to the best of your ability. If there's a tool you need, feel free to call it and use it.\n",
    "        # Start by breaking down your users' needs as finely as possible, and choose a tool to address each need.\n",
    "        # Answer in Korean.\"\"\",\n",
    "        #         ),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You're an AI programming assistant. You need to help users to the best of your ability.\n",
    "Your task is to figure out the overall structure of the project. Especially with complex projects, you'll want to keep track of how each file is related to the others.\n",
    "You should follow these steps\n",
    "1. Categorize and plan your users' needs in as much detail as possible. If necessary, choose a tool to address each need.\n",
    "2. process the plan sequentially.\n",
    "\n",
    "Answer in Korean.\n",
    "\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_directory_tree_str(root_dir):\n",
    "    directory_structure = \"<directory_structure>\\n\"\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        level = root.replace(root_dir, \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * (level)\n",
    "        directory_structure += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "        sub_indent = \" \" * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            directory_structure += f\"{sub_indent}{f}\\n\"\n",
    "    directory_structure += \"</directory_structure>\"\n",
    "    return directory_structure\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_directory_structure(root_dir: str) -> str:\n",
    "    \"\"\"Get the directory structure of the project. If you need to check the directory structure, use this tool.\"\"\"\n",
    "    return get_directory_tree_str(root_dir)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_file_contents(file_path: str) -> str:\n",
    "    \"\"\"Get the contents of a file. If you need to check the contents of each file, use this tool\"\"\"\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        # íŒŒì´ì¬ íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ê¸°\n",
    "        if file_path.endswith((\".py\", \".ipynb\")):\n",
    "            return f\"```python\\n{content}\\n```\"\n",
    "    return content\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "tools = [get_directory_structure, get_file_contents]\n",
    "\n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "def summary_agent(state: State) -> State:\n",
    "    messages = state[\"messages\"]\n",
    "    print(\"==================== [MESSAGES] ====================\")\n",
    "    print(messages)\n",
    "    response = chain.invoke({\"messages\": messages})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "work_flow = StateGraph(State)\n",
    "work_flow.add_node(\"agent\", summary_agent)\n",
    "work_flow.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "work_flow.add_edge(START, \"agent\")\n",
    "work_flow.add_edge(\"tool_node\", \"agent\")\n",
    "work_flow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,  #! ë³€ê²½ (return Literal[\"tools\", \"__end__\"])\n",
    "    {\"tools\": \"tool_node\", \"__end__\": END},\n",
    ")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = work_flow.compile(checkpointer=memory)\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ messages ]\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "main.py íŒŒì¼ì„ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
      "None\n",
      "==================== [MESSAGES] ====================\n",
      "[HumanMessage(content='ì „ì²´ í”„ë¡œì íŠ¸ ë¡œì§ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={}, id='49be0159-ec68-47f2-b7c7-65513c6e32a0'), AIMessage(content='í”„ë¡œì íŠ¸ì˜ ì „ì²´ ë¡œì§ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê³ , ê° íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ íŒŒì¼ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ íë¦„ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ì´ë¥¼ ìœ„í•œ ë‹¨ê³„ë³„ ê³„íšì…ë‹ˆë‹¤.\\n\\n1. **ë””ë ‰í† ë¦¬ êµ¬ì¡° íŒŒì•…**: í”„ë¡œì íŠ¸ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹œì‘í•˜ì—¬ ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ê°€ ì–´ë–¤ íŒŒì¼ê³¼ í´ë”ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€, ê° íŒŒì¼ì´ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n2. **íŒŒì¼ ë‚´ìš© í™•ì¸**: ê° íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ íŒŒì¼ì´ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€, ì–´ë–¤ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê³  ìˆëŠ”ì§€ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤. íŠ¹íˆ, ì£¼ìš” íŒŒì¼(ì˜ˆ: `main` íŒŒì¼, ì„¤ì • íŒŒì¼, ì£¼ìš” ëª¨ë“ˆ íŒŒì¼ ë“±)ì˜ ë‚´ìš©ì„ ì¤‘ì ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.\\n\\n3. **íŒŒì¼ ê°„ ê´€ê³„ ë¶„ì„**: íŒŒì¼ ê°„ì˜ ì˜ì¡´ì„±ì´ë‚˜ í˜¸ì¶œ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° íŒŒì¼ì´ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€, ì–´ë–¤ íŒŒì¼ì´ ë‹¤ë¥¸ íŒŒì¼ì„ í˜¸ì¶œí•˜ê±°ë‚˜ ì°¸ì¡°í•˜ëŠ”ì§€ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n4. **ì „ì²´ ë¡œì§ ì„¤ëª…**: ìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡œì íŠ¸ì˜ ì „ì²´ ë¡œì§ì„ ì„¤ëª…í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê¸°ëŠ¥, ë°ì´í„° íë¦„, ëª¨ë“ˆ ê°„ì˜ ìƒí˜¸ì‘ìš© ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.\\n\\nì´ì œ ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ `get_directory_structure` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ì‹œë©´, ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 348, 'prompt_tokens': 204, 'total_tokens': 552, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None}, id='run-35ec8929-cda1-4902-8ec0-26e2a5e57147-0', usage_metadata={'input_tokens': 204, 'output_tokens': 348, 'total_tokens': 552, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='ë„¤, /home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='c15802ea-12dd-4667-bb14-fb647053bc2c'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZS4ybx3BQjwA7toSWrf58rXN', 'function': {'arguments': '{\"root_dir\":\"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt\"}', 'name': 'get_directory_structure'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 576, 'total_tokens': 606, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f3cbf7c3-95e7-4758-8399-937a26b5ae01-0', tool_calls=[{'name': 'get_directory_structure', 'args': {'root_dir': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt'}, 'id': 'call_ZS4ybx3BQjwA7toSWrf58rXN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 576, 'output_tokens': 30, 'total_tokens': 606, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='<directory_structure>\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n</directory_structure>', name='get_directory_structure', id='497db54c-5e0f-4f75-b73f-328d4766e41f', tool_call_id='call_ZS4ybx3BQjwA7toSWrf58rXN'), AIMessage(content='ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ì œ ê° íŒŒì¼ê³¼ í´ë”ì˜ ì—­í• ì„ ì´í•´í•˜ê¸° ìœ„í•´ ì£¼ìš” íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ì˜ ì „ì²´ ë¡œì§ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” íŒŒì¼ë¡œëŠ” `app.py`, `main.py`, `graph/` ë””ë ‰í† ë¦¬ ë‚´ì˜ íŒŒì¼ë“¤, ê·¸ë¦¬ê³  `prompts/` ë””ë ‰í† ë¦¬ ë‚´ì˜ YAML íŒŒì¼ë“¤ì´ ìˆìŠµë‹ˆë‹¤. \\n\\në‹¤ìŒ ë‹¨ê³„ë¡œ, ì´ íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ í”„ë¡œì íŠ¸ì˜ êµ¬ì¡°ì™€ ë¡œì§ì„ ì´í•´í•˜ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ `get_file_contents` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤. ì£¼ìš” íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ë³‘ë ¬ë¡œ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.', additional_kwargs={'tool_calls': [{'id': 'call_P8aDhj8OcKQjORZP342KtowE', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_vi9Me8SHzlCE5SZJgL6P1tkM', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_QYXgGsKAyYHRvzhu2ZaDVvaj', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_99HdUTHi6Wbpvc7PGAY0ojdv', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_tJScz9GBDwO0oncOO5v98YK6', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_qesMqoGJSqMqA3KEf7D6YdOq', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_7KpU68z4YeZ4uRoKleOh8cAG', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/gather.yaml\"}', 'name': 'get_file_contents'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 392, 'prompt_tokens': 733, 'total_tokens': 1125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e25ad73a-ee59-4cf7-bd86-b0a36e7035ff-0', tool_calls=[{'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py'}, 'id': 'call_P8aDhj8OcKQjORZP342KtowE', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py'}, 'id': 'call_vi9Me8SHzlCE5SZJgL6P1tkM', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py'}, 'id': 'call_QYXgGsKAyYHRvzhu2ZaDVvaj', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py'}, 'id': 'call_99HdUTHi6Wbpvc7PGAY0ojdv', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py'}, 'id': 'call_tJScz9GBDwO0oncOO5v98YK6', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml'}, 'id': 'call_qesMqoGJSqMqA3KEf7D6YdOq', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/gather.yaml'}, 'id': 'call_7KpU68z4YeZ4uRoKleOh8cAG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 733, 'output_tokens': 392, 'total_tokens': 1125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='```python\\nimport streamlit as st\\nimport requests\\nimport json\\nimport time\\nimport uuid\\nfrom typing import Optional\\n\\nst.set_page_config(page_title=\"Meta Prompt Generator\", page_icon=\"ğŸ¤–\", layout=\"wide\")\\n\\nst.title(\"Meta Prompt Generator ğŸ¤–\")\\nst.markdown(\"LLMê³¼ ëŒ€í™”í•˜ë©° í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ë³´ì„¸ìš”.\")\\n\\n# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\n# thread_id ì´ˆê¸°í™” (ì„¸ì…˜ë³„ ê³ ìœ  ID)\\nif \"thread_id\" not in st.session_state:\\n    st.session_state.thread_id = str(uuid.uuid4())\\n\\n# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# ì‚¬ìš©ì ì…ë ¥\\nif prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”\"):\\n    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\\n    with st.chat_message(\"human\"):\\n        st.markdown(prompt)\\n\\n    # ë©”ì‹œì§€ ì €ì¥\\n    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\\n\\n    # AI ì‘ë‹µ ìƒì„±\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n\\n        try:\\n            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°\\n            with requests.get(\\n                \"http://localhost:8000/chat/stream\",\\n                params={\"query\": prompt, \"thread_id\": st.session_state.thread_id},\\n                headers={\"Accept\": \"text/event-stream\"},\\n                stream=True,\\n            ) as response:\\n                response.raise_for_status()\\n\\n                for line in response.iter_lines(decode_unicode=True):\\n                    if line.strip():\\n                        if line.startswith(\"data: \"):\\n                            try:\\n                                # JSON íŒŒì‹±\\n                                data = json.loads(line.split(\"data: \")[-1])\\n\\n                                if \"text\" in data:\\n                                    full_response += data[\"text\"]\\n                                    message_placeholder.markdown(full_response + \"â–Œ\")\\n                                elif \"error\" in data:\\n                                    st.error(f\"Error: {data[\\'error\\']}\")\\n                                    break\\n\\n                            except json.JSONDecodeError as e:\\n                                st.error(f\"JSON íŒŒì‹± ì—ëŸ¬: {e}\")\\n                                continue\\n\\n                            time.sleep(0.01)\\n\\n            # ìµœì¢… ì‘ë‹µ í‘œì‹œ\\n            message_placeholder.markdown(full_response)\\n\\n            # AI ì‘ë‹µ ì €ì¥\\n            st.session_state.messages.append(\\n                {\"role\": \"assistant\", \"content\": full_response}\\n            )\\n\\n        except requests.exceptions.RequestException as e:\\n            st.error(f\"ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {str(e)}\")\\n        except Exception as e:\\n            st.error(f\"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}\")\\n\\n# ë””ë²„ê¹…ìš© ì„¸ì…˜ ì •ë³´ í‘œì‹œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\\n# st.sidebar.write(\"Session Thread ID:\", st.session_state.thread_id)\\n\\n```', name='get_file_contents', id='8062cfb0-c500-4a94-bcc1-e9e2ee12e976', tool_call_id='call_P8aDhj8OcKQjORZP342KtowE'), ToolMessage(content='```python\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import StreamingResponse\\nfrom langchain_core.messages import HumanMessage\\nfrom graph.graph import graph\\nimport json\\n\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/chat/stream\")\\nasync def stream(query: str, thread_id: str = \"1\"):\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n    async def event_stream():\\n        try:\\n            async for chunk in graph.astream(\\n                input={\"messages\": [HumanMessage(content=query)]},\\n                config=config,\\n                stream_mode=[\"custom\"],\\n            ):\\n                yield f\"data: {json.dumps({\\'text\\': chunk[1]})}\\\\n\\\\n\"\\n        except Exception as e:\\n            print(f\"========== {e}\")\\n            yield f\"data: {json.dumps({\\'error\\': str(e)})}\\\\n\\\\n\"\\n\\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\\n\\n```', name='get_file_contents', id='0a1d97d4-8c62-4513-a308-02d3df4bbc1a', tool_call_id='call_vi9Me8SHzlCE5SZJgL6P1tkM'), ToolMessage(content='```python\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated, TypedDict\\n\\nfrom graph.nodes import information_gather, generate_prompt, add_tool_message\\nfrom graph.edges import get_state\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\n\\nmemory = MemorySaver()\\n\\nflow = StateGraph(State)\\nflow.add_node(\"information_gather\", information_gather)\\nflow.add_node(\"generate_prompt\", generate_prompt)\\nflow.add_node(\"add_tool_message\", add_tool_message)\\n\\nflow.add_edge(START, \"information_gather\")\\nflow.add_conditional_edges(\\n    \"information_gather\",\\n    get_state,\\n    {\\n        \"add_tool_message\": \"add_tool_message\",\\n        END: END,\\n    },\\n)\\nflow.add_edge(\"add_tool_message\", \"generate_prompt\")\\nflow.add_edge(\"generate_prompt\", END)\\n\\n\\ngraph = flow.compile(checkpointer=memory)\\n\\n\\ngraph.get_graph().draw_mermaid_png(output_file_path=\"./graph.png\")\\n\\n```', name='get_file_contents', id='17ab4a79-3233-4d08-b385-9a9712a3d073', tool_call_id='call_QYXgGsKAyYHRvzhu2ZaDVvaj'), ToolMessage(content='```python\\nfrom langgraph.graph import END\\n\\nfrom langchain_core.messages import AIMessage\\n\\n\\ndef get_state(state):\\n    messages = state[\"messages\"]\\n    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\\n        return \"add_tool_message\"\\n    return END\\n\\n```', name='get_file_contents', id='e0535fc2-06b4-459c-9dfa-ce3576858f7e', tool_call_id='call_99HdUTHi6Wbpvc7PGAY0ojdv'), ToolMessage(content='```python\\nimport yaml\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_core.messages import SystemMessage, AIMessage, ToolMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.types import StreamWriter\\nfrom pydantic import BaseModel\\nfrom typing import List\\n\\nload_dotenv()\\nMODEL_NAME = os.getenv(\"MODEL_NAME\")\\nTEMPERATURE = os.getenv(\"TEMPERATURE\")\\n\\n# * information gather\\n\\nwith open(\"./prompts/gather.yaml\", \"r\") as f:\\n    gather_template = yaml.safe_load(f)\\n\\ngather_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (gather_template[\"role\"], gather_template[\"template\"]),\\n        MessagesPlaceholder(variable_name=\"messages\"),\\n    ]\\n)\\n\\n\\nclass PromptInstructions(BaseModel):\\n    \"\"\"Instructions on how to prompt the LLM.\"\"\"\\n\\n    objective: str\\n    variables: List[str]\\n    constraints: List[str]\\n    requirements: List[str]\\n\\n\\nllm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\\nllm_with_tool = llm.bind_tools([PromptInstructions])\\n\\ngather_chain = gather_prompt | llm_with_tool\\n\\n\\nasync def information_gather(state, writer: StreamWriter):\\n    \"\"\"ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ë©° í•„ìš”í•œ ìš”êµ¬ ì‚¬í•­ì„ ìˆ˜ì§‘í•˜ëŠ” ë…¸ë“œ\"\"\"\\n    print(\"--- [Information_gather] ---\")\\n    messages = state[\"messages\"]\\n    print(messages)\\n    response = []\\n    tool_calls = None\\n\\n    first = True\\n    async for chunk in gather_chain.astream({\"messages\": messages}):\\n        if chunk.tool_call_chunks:\\n            if first:\\n                tool_calls = chunk\\n                first = False\\n            else:\\n                tool_calls += chunk\\n        elif chunk.content:\\n            response.append(chunk.content)\\n            writer(chunk.content)\\n        else:\\n            continue\\n\\n    if response:\\n        response = AIMessage(content=\"\".join(response))\\n        return {\"messages\": [response]}\\n    else:\\n        return {\"messages\": [tool_calls]}\\n\\n\\n# * generate prompt\\nwith open(\"./prompts/generate_prompt.yaml\", \"r\") as f:\\n    generate_template = yaml.safe_load(f)\\n\\n\\ndef get_prompt_messages(messages: list):\\n    tool_call = None\\n    other_messages = []\\n\\n    for msg in messages:\\n        if isinstance(msg, AIMessage) and msg.tool_calls:\\n            tool_call = msg.tool_calls[0][\"args\"]\\n        elif isinstance(msg, ToolMessage):\\n            continue\\n        # tool call ì´í›„ì˜ ë©”ì„¸ì§€ ì¶”ê°€ (ì¶”ê°€ì ì¸ ëŒ€í™” ë‚´ìš©)\\n        elif tool_call is not None:\\n            other_messages.append(msg)\\n    return [\\n        SystemMessage(\\n            content=generate_template[\"template\"].format(requirements=tool_call)\\n        )\\n    ] + other_messages\\n\\n\\nasync def generate_prompt(state, writer: StreamWriter):\\n    print(\"--- [GENERATE PROMPT] ---\")\\n    messages = get_prompt_messages(state[\"messages\"])\\n    print(messages)\\n    response = []\\n    async for chunk in llm.astream(messages):\\n        response.append(chunk.content)\\n        # print(chunk)\\n        # print(\"---\")\\n        writer(chunk.content)\\n\\n    return {\"messages\": [AIMessage(content=\"\".join(response))]}\\n\\n\\n# * add_tool_message ë…¸ë“œ ì •ì˜\\ndef add_tool_message(state):\\n    return {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"prompt ìƒì„± ì™„ë£Œ!\",\\n                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\\n            )\\n        ]\\n    }\\n\\n\\nif __name__ == \"__main__\":\\n    print(\\n        gather_chain.invoke(\\n            {\"messages\": [(\"user\", \"ì•ˆë…•í•˜ì„¸ìš”, í”„ë¡¬í”„íŠ¸ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\")]}\\n        ).content\\n    )\\n\\n```', name='get_file_contents', id='b65f3143-ec3c-4f69-a337-a87ce4efb9a7', tool_call_id='call_tJScz9GBDwO0oncOO5v98YK6'), ToolMessage(content='role: \"system\"\\ntemplate: |\\n  You are an expert prompt engineer. Your task is to generate a well-structured, detailed, and effective prompt based on a given set of requirements.\\n\\n  ## Instructions:\\n  - Read and understand the provided requirements carefully.\\n  - Ensure the generated prompt is clear, specific, and optimized for an LLM response.\\n  - Use precise language and structure the prompt logically.\\n  - If the requirements are ambiguous, make reasonable assumptions and clarify them in the prompt.\\n  - If necessary, break the prompt into multiple steps to guide the model\\'s response effectively.\\n\\n  ## [IMPORTANT]\\n  - variables must be expressed in {{variable_name}} format with curly braces.\\n\\n\\n  ## Requirements:\\n  {requirements}\\n\\n  Now, generate the best possible prompt based on the above requirements.\\ninput_variables: [\"requirements\"]', name='get_file_contents', id='0ddc2cff-c6a6-4bfd-8746-ee7a81821478', tool_call_id='call_qesMqoGJSqMqA3KEf7D6YdOq'), ToolMessage(content='role: \"system\"\\ntemplate: |\\n  Your job is to get information from a user about what type of prompt template they want to create.\\n\\n  You should get the following information from them:\\n\\n  - What the objective of the prompt is\\n  - What variables will be passed into the prompt template\\n  - Any constraints for what the output should NOT do\\n  - Any requirements that the output MUST adhere to\\n\\n  [IMPORTANT!]\\n  - You must communicate in Korean. However, the prompt generation must be in English.\\n  - If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\\n  - After you are able to discern all the information, call the relevant tool.\\ninput_variables: []', name='get_file_contents', id='cccc94a2-6d8d-4d77-ba13-9d440d885f0f', tool_call_id='call_7KpU68z4YeZ4uRoKleOh8cAG'), AIMessage(content='í”„ë¡œì íŠ¸ì˜ ì£¼ìš” íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê° íŒŒì¼ì˜ ì—­í• ê³¼ í”„ë¡œì íŠ¸ì˜ ì „ì²´ ë¡œì§ì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n1. **`app.py`**: \\n   - Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì„±í•©ë‹ˆë‹¤. \\n   - ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µì„ ìƒì„±í•˜ê³ , ì´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.\\n   - ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ì—¬ ì´ì „ ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê³ , ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\\n\\n2. **`main.py`**:\\n   - FastAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\\n   - `/chat/stream` ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•´ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.\\n   - `graph` ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\\n\\n3. **`graph/graph.py`**:\\n   - `StateGraph`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\\n   - ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì¶”ê°€í•˜ì—¬ ì •ë³´ ìˆ˜ì§‘, í”„ë¡¬í”„íŠ¸ ìƒì„±, ë„êµ¬ ë©”ì‹œì§€ ì¶”ê°€ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\\n   - ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.\\n\\n4. **`graph/edges.py`**:\\n   - ìƒíƒœì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•˜ëŠ” ë¡œì§ì„ ì •ì˜í•©ë‹ˆë‹¤.\\n   - AI ë©”ì‹œì§€ì— ë„êµ¬ í˜¸ì¶œì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ ë‹¤ìŒ ìƒíƒœë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\\n\\n5. **`graph/nodes.py`**:\\n   - ì •ë³´ ìˆ˜ì§‘, í”„ë¡¬í”„íŠ¸ ìƒì„±, ë„êµ¬ ë©”ì‹œì§€ ì¶”ê°€ ë“±ì˜ ë…¸ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\\n   - ê° ë…¸ë“œëŠ” ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\\n\\n6. **`prompts/generate_prompt.yaml` ë° `prompts/gather.yaml`**:\\n   - í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•œ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\\n   - `generate_prompt.yaml`ì€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œ ì‚¬ìš©í•  í…œí”Œë¦¿ì„, `gather.yaml`ì€ ì‚¬ìš©ìë¡œë¶€í„° ì •ë³´ë¥¼ ìˆ˜ì§‘í•  ë•Œ ì‚¬ìš©í•  í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\\n\\n**ì „ì²´ ë¡œì§**:\\n- ì‚¬ìš©ìê°€ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ í†µí•´ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ë©´, ì´ ë©”ì‹œì§€ëŠ” FastAPI ì„œë²„ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\\n- ì„œë²„ëŠ” `graph` ëª¨ë“ˆì„ í†µí•´ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³ , ìƒíƒœ ê·¸ë˜í”„ì— ë”°ë¼ ì •ë³´ ìˆ˜ì§‘, í”„ë¡¬í”„íŠ¸ ìƒì„± ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\\n- ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ëŠ” ì‚¬ìš©ìì—ê²Œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ëŠ” ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 570, 'prompt_tokens': 4181, 'total_tokens': 4751, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1024}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3c0e617e-a293-427b-be79-a7d541d27d35-0', usage_metadata={'input_tokens': 4181, 'output_tokens': 570, 'total_tokens': 4751, 'input_token_details': {'audio': 0, 'cache_read': 1024}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='í•´ë‹¹ íŒŒì¼ ê°„ì˜ ì—°ê²° ê´€ê³„ë¥¼ ì¢€ ë” ëª…í™•íˆ ì‹œê°í™”í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?', additional_kwargs={}, response_metadata={}, id='4e20454b-fed8-4753-a731-8a363c823d5c'), AIMessage(content='í”„ë¡œì íŠ¸ì˜ íŒŒì¼ ê°„ ì—°ê²° ê´€ê³„ë¥¼ ëª…í™•íˆ ì‹œê°í™”í•˜ê¸° ìœ„í•´ ê° íŒŒì¼ì´ ì–´ë–¤ ì—­í• ì„ í•˜ê³ , ë‹¤ë¥¸ íŒŒì¼ê³¼ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ì˜ êµ¬ì¡°ë¥¼ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n1. **`app.py`**:\\n   - **ì—­í• **: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜.\\n   - **ì—°ê²°**: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ `main.py`ì˜ FastAPI ì„œë²„ë¡œ ì „ì†¡í•˜ì—¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°›ìŠµë‹ˆë‹¤.\\n\\n2. **`main.py`**:\\n   - **ì—­í• **: FastAPI ì„œë²„ë¡œ, í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ìš”ì²­ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\\n   - **ì—°ê²°**: `graph/graph.py`ì˜ `graph` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\\n\\n3. **`graph/graph.py`**:\\n   - **ì—­í• **: ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì •ì˜í•˜ê³  ì»´íŒŒì¼í•˜ì—¬ ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\\n   - **ì—°ê²°**: `graph/nodes.py`ì™€ `graph/edges.py`ì˜ ë…¸ë“œ ë° ì—£ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\\n\\n4. **`graph/edges.py`**:\\n   - **ì—­í• **: ìƒíƒœ ì „í™˜ ë¡œì§ì„ ì •ì˜í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\\n   - **ì—°ê²°**: `graph/graph.py`ì—ì„œ ìƒíƒœ ì „í™˜ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\n5. **`graph/nodes.py`**:\\n   - **ì—­í• **: ì •ë³´ ìˆ˜ì§‘, í”„ë¡¬í”„íŠ¸ ìƒì„± ë“±ì˜ ë…¸ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\\n   - **ì—°ê²°**: `graph/graph.py`ì—ì„œ ê° ë…¸ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\\n\\n6. **`prompts/generate_prompt.yaml` ë° `prompts/gather.yaml`**:\\n   - **ì—­í• **: í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•œ í…œí”Œë¦¿ì„ ì œê³µí•©ë‹ˆë‹¤.\\n   - **ì—°ê²°**: `graph/nodes.py`ì—ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\n**ì‹œê°í™”**:\\n- `app.py`ëŠ” ì‚¬ìš©ìì™€ì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë‹´ë‹¹í•˜ë©°, `main.py`ë¡œ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.\\n- `main.py`ëŠ” `graph/graph.py`ì˜ `graph` ê°ì²´ë¥¼ í˜¸ì¶œí•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\\n- `graph/graph.py`ëŠ” `graph/nodes.py`ì™€ `graph/edges.py`ì˜ ì •ì˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\\n- `graph/nodes.py`ëŠ” `prompts/generate_prompt.yaml` ë° `prompts/gather.yaml`ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ ì—°ê²° ê´€ê³„ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ëŠ” ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 586, 'prompt_tokens': 4780, 'total_tokens': 5366, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4736}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3ac76c31-f0f2-475d-a6ec-7422c0c0173c-0', usage_metadata={'input_tokens': 4780, 'output_tokens': 586, 'total_tokens': 5366, 'input_token_details': {'audio': 0, 'cache_read': 4736}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='main.py íŒŒì¼ì„ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={}, id='96c02f15-b951-4772-a357-0e596f36ac60')]\n",
      "\n",
      "[ messages ]\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "`main.py` íŒŒì¼ì€ FastAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ë¥¼ êµ¬ì„±í•˜ê³ , í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„°ì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ íŒŒì¼ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œì™€ ë™ì‘ì„ ìì„¸íˆ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n",
      "\n",
      "1. **FastAPI ì•± ì´ˆê¸°í™”**:\n",
      "   ```python\n",
      "   app = FastAPI()\n",
      "   ```\n",
      "   - FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í†µí•´ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì •ì˜í•˜ê³  ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **ì—”ë“œí¬ì¸íŠ¸ ì •ì˜**:\n",
      "   ```python\n",
      "   @app.get(\"/chat/stream\")\n",
      "   async def stream(query: str, thread_id: str = \"1\"):\n",
      "   ```\n",
      "   - `/chat/stream` ê²½ë¡œì— ëŒ€í•œ GET ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
      "   - `query`ì™€ `thread_id`ë¼ëŠ” ë‘ ê°œì˜ ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤. `query`ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ì´ê³ , `thread_id`ëŠ” ì„¸ì…˜ì„ ì‹ë³„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ìƒì„±**:\n",
      "   ```python\n",
      "   async def event_stream():\n",
      "       try:\n",
      "           async for chunk in graph.astream(\n",
      "               input={\"messages\": [HumanMessage(content=query)]},\n",
      "               config=config,\n",
      "               stream_mode=[\"custom\"],\n",
      "           ):\n",
      "               yield f\"data: {json.dumps({'text': chunk[1]})}\\n\\n\"\n",
      "       except Exception as e:\n",
      "           print(f\"========== {e}\")\n",
      "           yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n",
      "   ```\n",
      "   - `event_stream` í•¨ìˆ˜ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ, `graph.astream`ì„ í˜¸ì¶œí•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\n",
      "   - ê° ì²­í¬(chunk)ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì–´ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\n",
      "   - ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **StreamingResponse ë°˜í™˜**:\n",
      "   ```python\n",
      "   return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n",
      "   ```\n",
      "   - `StreamingResponse` ê°ì²´ë¥¼ ë°˜í™˜í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "   - `event_stream` í•¨ìˆ˜ì—ì„œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ë™ì‘ ì„¤ëª…\n",
      "\n",
      "- í´ë¼ì´ì–¸íŠ¸ê°€ `/chat/stream` ì—”ë“œí¬ì¸íŠ¸ì— GET ìš”ì²­ì„ ë³´ë‚´ë©´, `stream` í•¨ìˆ˜ê°€ í˜¸ì¶œë©ë‹ˆë‹¤.\n",
      "- `stream` í•¨ìˆ˜ëŠ” `event_stream` ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ í†µí•´ `graph` ê°ì²´ì˜ `astream` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "- ì²˜ë¦¬ëœ ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì–´ `StreamingResponse`ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\n",
      "- ì´ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜ˆì™¸ëŠ” ì¡ì•„ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¡œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤.\n",
      "\n",
      "`main.py` íŒŒì¼ì€ FastAPIë¥¼ í™œìš©í•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³ , ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œê³µí•˜ëŠ” í•µì‹¬ì ì¸ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "config = {\"recursion_limit\": 10, \"configurable\": {\"thread_id\": \"abc\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"main.py íŒŒì¼ì„ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")]}\n",
    "\n",
    "# stream_graph(graph, inputs, config)\n",
    "\n",
    "for event in graph.stream(\n",
    "    input=inputs,\n",
    "    config=config,\n",
    "    stream_mode=\"values\",  # ê° ë…¸ë“œì˜ ì¶œë ¥ ìƒíƒœ\n",
    "    # interrupt_before=[\"tool_node\"],  # tool_nodeë¡œ ê°€ê¸° ì „ì— ë©ˆì¶”ê¸°!\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n[ {key} ]\\n\")\n",
    "\n",
    "        print(value[-1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='ì „ì²´ í”„ë¡œì íŠ¸ ë¡œì§ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={}, id='cfb41c39-66b3-4cc9-a8f8-6f371885db01'),\n",
       "  AIMessage(content='í”„ë¡œì íŠ¸ì˜ ì „ì²´ ë¡œì§ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ì„œëŠ” í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì™€ ê° íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ìš°ì„  í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤. ì–´ëŠ ë””ë ‰í† ë¦¬ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í• ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 221, 'total_tokens': 276, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'stop', 'logprobs': None}, id='run-80a001bf-9a0a-4e0e-a9c4-4d74083dc412-0', usage_metadata={'input_tokens': 221, 'output_tokens': 55, 'total_tokens': 276, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='ë„¤, /home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}, id='458df4a5-318f-48a0-ba29-d0064c03195d'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sB771upNNtW89bjiYexVYtzA', 'function': {'arguments': '{\"root_dir\":\"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt\"}', 'name': 'get_directory_structure'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 300, 'total_tokens': 330, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-139b86d7-b757-48b3-b071-76e4bb2e4f9e-0', tool_calls=[{'name': 'get_directory_structure', 'args': {'root_dir': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt'}, 'id': 'call_sB771upNNtW89bjiYexVYtzA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 300, 'output_tokens': 30, 'total_tokens': 330, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='<directory_structure>\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n</directory_structure>', name='get_directory_structure', id='3bf059b5-b7d6-45fb-a0b0-1217323a80dd', tool_call_id='call_sB771upNNtW89bjiYexVYtzA'),\n",
       "  AIMessage(content='ë””ë ‰í† ë¦¬ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n- `meta_prompt/`\\n  - `.env`\\n  - `requirements.txt`\\n  - `README.md`\\n  - `graph.png`\\n  - `app.py`\\n  - `.env.example`\\n  - `main.py`\\n  - `Dockerfile`\\n  - `graph/`\\n    - `graph.py`\\n    - `edges.py`\\n    - `nodes.py`\\n  - `prompts/`\\n    - `generate_prompt.yaml`\\n    - `gather.yaml`\\n\\nì´ì œ ê° íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ ì „ì²´ ë¡œì§ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ íŒŒì¼ë¶€í„° í™•ì¸í• ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 457, 'total_tokens': 591, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a761218-ee79-47cf-b4b5-67f9d4c6e40a-0', usage_metadata={'input_tokens': 457, 'output_tokens': 134, 'total_tokens': 591, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='ì•Œì•„ì„œ íŒë‹¨í•´ì£¼ì„¸ìš”', additional_kwargs={}, response_metadata={}, id='359207a8-2724-4bfd-8f0b-87dca730b84a'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_qZkmhAUhkkeYm6k1MM79j8Tq', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/README.md\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_LGIiR9bxGOK70GnzxbXyn1V3', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_JeuTdSGssrNziVUW55K0jfYw', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_FqftYvEoNy15oSWig0M16iJB', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_W2mi7iQpuriuePYBQhvPAr7m', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml\"}', 'name': 'get_file_contents'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 601, 'total_tokens': 781, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-6a6cbde4-9149-4cf1-91ea-6f45f91e005a-0', tool_calls=[{'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/README.md'}, 'id': 'call_qZkmhAUhkkeYm6k1MM79j8Tq', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py'}, 'id': 'call_LGIiR9bxGOK70GnzxbXyn1V3', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py'}, 'id': 'call_JeuTdSGssrNziVUW55K0jfYw', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py'}, 'id': 'call_FqftYvEoNy15oSWig0M16iJB', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml'}, 'id': 'call_W2mi7iQpuriuePYBQhvPAr7m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 601, 'output_tokens': 180, 'total_tokens': 781, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='# Meta-Prompt Generator\\n\\nLangGraphë¥¼ í™œìš©í•œ ëŒ€í™”í˜• ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\\n\\n## ì£¼ìš” ê¸°ëŠ¥\\n\\n- ğŸ¤– ëŒ€í™”í˜• ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„±\\n- ğŸŒŠ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ\\n- ğŸ“ í”„ë¡¬í”„íŠ¸ ìµœì í™” ë° ê²€ì¦\\n- ğŸ¯ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\\n- ğŸ”„ LangGraph ê¸°ë°˜ì˜ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\\n- ğŸŒ Streamlit ê¸°ë°˜ì˜ ì›¹ ì¸í„°í˜ì´ìŠ¤\\n\\n## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜\\n\\n![System Architecture](graph.png)\\n\\n## ê¸°ìˆ  ìŠ¤íƒ\\n\\n- ğŸ Python 3.11+\\n- âš¡ FastAPI\\n- ğŸ”„ LangGraph\\n- ğŸ¦œ LangChain\\n- ğŸˆ Streamlit\\n- ğŸ³ Docker\\n\\n## ì‹œì‘í•˜ê¸°\\n\\n### ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰\\n\\n1. í™˜ê²½ ì„¤ì •\\n```bash\\n# ì €ì¥ì†Œ í´ë¡ \\ngit clone https://github.com/yourusername/meta-prompt-generator.git\\ncd meta-prompt-generator\\n\\n# ê°€ìƒí™˜ê²½ ìƒì„±\\npython -m venv venv\\nsource venv/bin/activate  # Windows: venv\\\\Scripts\\\\activate\\n\\n# ì˜ì¡´ì„± ì„¤ì¹˜\\npip install -r requirements.txt\\n\\n# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\\ncp .env.example .env\\n# .env íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í•„ìš”í•œ API í‚¤ ì„¤ì •\\n```\\n\\n2. ì„œë²„ ì‹¤í–‰\\n```bash\\n# FastAPI ì„œë²„ ì‹¤í–‰\\nuvicorn main:app --reload --port 8000\\n\\n# ìƒˆë¡œìš´ í„°ë¯¸ë„ì—ì„œ Streamlit ì•± ì‹¤í–‰\\nstreamlit run app.py\\n```\\n\\n### Dockerë¡œ ì‹¤í–‰\\n\\n```bash\\n# ì´ë¯¸ì§€ ë¹Œë“œ\\ndocker build -t meta-prompt-generator .\\n\\n# ì»¨í…Œì´ë„ˆ ì‹¤í–‰\\ndocker run -p 8000:8000 -p 8501:8501 meta-prompt-generator\\n```\\n\\n## ì‚¬ìš© ë°©ë²•\\n\\n1. ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8501` ì ‘ì†\\n2. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì—ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„± ìš”êµ¬ì‚¬í•­ ì…ë ¥\\n3. ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±ë˜ëŠ” í”„ë¡¬í”„íŠ¸ í™•ì¸ ë° ìˆ˜ì •\\n\\n## API ì—”ë“œí¬ì¸íŠ¸\\n\\n- GET `/chat/stream`: ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… API\\n  - Parameters:\\n    - `query`: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸\\n    - `thread_id`: ì±„íŒ… ìŠ¤ë ˆë“œ ID (ì„ íƒì‚¬í•­)\\n\\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\\n\\n```\\n.\\nâ”œâ”€â”€ app.py              # Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤\\nâ”œâ”€â”€ main.py            # FastAPI ì„œë²„\\nâ”œâ”€â”€ graph/             # LangGraph ê´€ë ¨ ì½”ë“œ\\nâ”‚   â”œâ”€â”€ graph.py      # ë©”ì¸ ê·¸ë˜í”„ ì •ì˜\\nâ”‚   â”œâ”€â”€ nodes.py      # ê·¸ë˜í”„ ë…¸ë“œ ì •ì˜\\nâ”‚   â””â”€â”€ edges.py      # ê·¸ë˜í”„ ì—£ì§€ ì •ì˜\\nâ”œâ”€â”€ prompts/          # YAML í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\\nâ”œâ”€â”€ requirements.txt  # í”„ë¡œì íŠ¸ ì˜ì¡´ì„±\\nâ”œâ”€â”€ Dockerfile       # ì»¨í…Œì´ë„ˆí™” ì„¤ì •\\nâ””â”€â”€ README.md       # í”„ë¡œì íŠ¸ ë¬¸ì„œ\\n```\\n\\n## í™˜ê²½ ë³€ìˆ˜\\n\\n`.env` íŒŒì¼ì— ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ gpt-4o ì…ë‹ˆë‹¤.\\n\\n```\\nOPENAI_API_KEY=your_openai_api_key\\nMODEL_NAME=gpt-4o\\nTEMPERATURE=0.7\\n```\\n', name='get_file_contents', id='dc7663a2-9820-46a5-83c3-183c7c212d7e', tool_call_id='call_qZkmhAUhkkeYm6k1MM79j8Tq'),\n",
       "  ToolMessage(content='import streamlit as st\\nimport requests\\nimport json\\nimport time\\nimport uuid\\nfrom typing import Optional\\n\\nst.set_page_config(page_title=\"Meta Prompt Generator\", page_icon=\"ğŸ¤–\", layout=\"wide\")\\n\\nst.title(\"Meta Prompt Generator ğŸ¤–\")\\nst.markdown(\"LLMê³¼ ëŒ€í™”í•˜ë©° í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ë³´ì„¸ìš”.\")\\n\\n# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\n# thread_id ì´ˆê¸°í™” (ì„¸ì…˜ë³„ ê³ ìœ  ID)\\nif \"thread_id\" not in st.session_state:\\n    st.session_state.thread_id = str(uuid.uuid4())\\n\\n# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# ì‚¬ìš©ì ì…ë ¥\\nif prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”\"):\\n    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\\n    with st.chat_message(\"human\"):\\n        st.markdown(prompt)\\n\\n    # ë©”ì‹œì§€ ì €ì¥\\n    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\\n\\n    # AI ì‘ë‹µ ìƒì„±\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n\\n        try:\\n            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°\\n            with requests.get(\\n                \"http://localhost:8000/chat/stream\",\\n                params={\"query\": prompt, \"thread_id\": st.session_state.thread_id},\\n                headers={\"Accept\": \"text/event-stream\"},\\n                stream=True,\\n            ) as response:\\n                response.raise_for_status()\\n\\n                for line in response.iter_lines(decode_unicode=True):\\n                    if line.strip():\\n                        if line.startswith(\"data: \"):\\n                            try:\\n                                # JSON íŒŒì‹±\\n                                data = json.loads(line.split(\"data: \")[-1])\\n\\n                                if \"text\" in data:\\n                                    full_response += data[\"text\"]\\n                                    message_placeholder.markdown(full_response + \"â–Œ\")\\n                                elif \"error\" in data:\\n                                    st.error(f\"Error: {data[\\'error\\']}\")\\n                                    break\\n\\n                            except json.JSONDecodeError as e:\\n                                st.error(f\"JSON íŒŒì‹± ì—ëŸ¬: {e}\")\\n                                continue\\n\\n                            time.sleep(0.01)\\n\\n            # ìµœì¢… ì‘ë‹µ í‘œì‹œ\\n            message_placeholder.markdown(full_response)\\n\\n            # AI ì‘ë‹µ ì €ì¥\\n            st.session_state.messages.append(\\n                {\"role\": \"assistant\", \"content\": full_response}\\n            )\\n\\n        except requests.exceptions.RequestException as e:\\n            st.error(f\"ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {str(e)}\")\\n        except Exception as e:\\n            st.error(f\"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}\")\\n\\n# ë””ë²„ê¹…ìš© ì„¸ì…˜ ì •ë³´ í‘œì‹œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\\n# st.sidebar.write(\"Session Thread ID:\", st.session_state.thread_id)\\n', name='get_file_contents', id='fe9d1657-022f-4461-ad60-4ec8610b1289', tool_call_id='call_LGIiR9bxGOK70GnzxbXyn1V3'),\n",
       "  ToolMessage(content='from fastapi import FastAPI\\nfrom fastapi.responses import StreamingResponse\\nfrom langchain_core.messages import HumanMessage\\nfrom graph.graph import graph\\nimport json\\n\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/chat/stream\")\\nasync def stream(query: str, thread_id: str = \"1\"):\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n    async def event_stream():\\n        try:\\n            async for chunk in graph.astream(\\n                input={\"messages\": [HumanMessage(content=query)]},\\n                config=config,\\n                stream_mode=[\"custom\"],\\n            ):\\n                yield f\"data: {json.dumps({\\'text\\': chunk[1]})}\\\\n\\\\n\"\\n        except Exception as e:\\n            print(f\"========== {e}\")\\n            yield f\"data: {json.dumps({\\'error\\': str(e)})}\\\\n\\\\n\"\\n\\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\\n', name='get_file_contents', id='51ec0f04-6200-40d8-88dc-d82d45a8e56d', tool_call_id='call_JeuTdSGssrNziVUW55K0jfYw'),\n",
       "  ToolMessage(content='from langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated, TypedDict\\n\\nfrom graph.nodes import information_gather, generate_prompt, add_tool_message\\nfrom graph.edges import get_state\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\n\\nmemory = MemorySaver()\\n\\nflow = StateGraph(State)\\nflow.add_node(\"information_gather\", information_gather)\\nflow.add_node(\"generate_prompt\", generate_prompt)\\nflow.add_node(\"add_tool_message\", add_tool_message)\\n\\nflow.add_edge(START, \"information_gather\")\\nflow.add_conditional_edges(\\n    \"information_gather\",\\n    get_state,\\n    {\\n        \"add_tool_message\": \"add_tool_message\",\\n        END: END,\\n    },\\n)\\nflow.add_edge(\"add_tool_message\", \"generate_prompt\")\\nflow.add_edge(\"generate_prompt\", END)\\n\\n\\ngraph = flow.compile(checkpointer=memory)\\n\\n\\ngraph.get_graph().draw_mermaid_png(output_file_path=\"./graph.png\")\\n', name='get_file_contents', id='2f2df6ec-e028-436f-bc56-ac52a1842a34', tool_call_id='call_FqftYvEoNy15oSWig0M16iJB'),\n",
       "  ToolMessage(content='role: \"system\"\\ntemplate: |\\n  You are an expert prompt engineer. Your task is to generate a well-structured, detailed, and effective prompt based on a given set of requirements.\\n\\n  ## Instructions:\\n  - Read and understand the provided requirements carefully.\\n  - Ensure the generated prompt is clear, specific, and optimized for an LLM response.\\n  - Use precise language and structure the prompt logically.\\n  - If the requirements are ambiguous, make reasonable assumptions and clarify them in the prompt.\\n  - If necessary, break the prompt into multiple steps to guide the model\\'s response effectively.\\n\\n  ## [IMPORTANT]\\n  - variables must be expressed in {{variable_name}} format with curly braces.\\n\\n\\n  ## Requirements:\\n  {requirements}\\n\\n  Now, generate the best possible prompt based on the above requirements.\\ninput_variables: [\"requirements\"]', name='get_file_contents', id='01e5dd1f-7f3b-4bf6-94ba-44862792d8b6', tool_call_id='call_W2mi7iQpuriuePYBQhvPAr7m'),\n",
       "  AIMessage(content='í”„ë¡œì íŠ¸ì˜ ì „ì²´ ë¡œì§ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. **í”„ë¡œì íŠ¸ ê°œìš”**:\\n   - ì´ í”„ë¡œì íŠ¸ëŠ” LangGraphë¥¼ í™œìš©í•˜ì—¬ ëŒ€í™”í˜• ë©”íƒ€ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n2. **ì£¼ìš” êµ¬ì„± ìš”ì†Œ**:\\n   - `app.py`: Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ FastAPI ì„œë²„ë¡œ ì „ì†¡í•˜ê³ , ì„œë²„ë¡œë¶€í„°ì˜ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.\\n   - `main.py`: FastAPI ì„œë²„ë¡œ, ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë°›ì•„ LangGraphë¥¼ í†µí•´ ì²˜ë¦¬í•˜ê³  ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.\\n   - `graph/graph.py`: LangGraphì˜ ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì •ì˜í•˜ê³ , ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì¶”ê°€í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„± íë¦„ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\\n   - `prompts/generate_prompt.yaml`: í”„ë¡¬í”„íŠ¸ ìƒì„±ì— í•„ìš”í•œ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n\\n3. **ê¸°ìˆ  ìŠ¤íƒ**:\\n   - Python 3.11+, FastAPI, LangGraph, LangChain, Streamlit, Docker ë“±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n\\n4. **ì‹¤í–‰ ë°©ë²•**:\\n   - ë¡œì»¬ í™˜ê²½ì—ì„œ Python ê°€ìƒ í™˜ê²½ì„ ì„¤ì •í•˜ê³  í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œ í›„, FastAPI ì„œë²„ì™€ Streamlit ì•±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\\n   - Dockerë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…Œì´ë„ˆí™”ëœ í™˜ê²½ì—ì„œë„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n5. **ì‚¬ìš© ë°©ë²•**:\\n   - ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8501`ì— ì ‘ì†í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„± ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ê³ , ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nì´ í”„ë¡œì íŠ¸ëŠ” ëŒ€í™”í˜•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 408, 'prompt_tokens': 4584, 'total_tokens': 4992, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a0e0e19-b79a-4a8f-a7f8-2d1669908564-0', usage_metadata={'input_tokens': 4584, 'output_tokens': 408, 'total_tokens': 4992, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
