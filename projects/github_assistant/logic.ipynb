{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"github_assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #         (\n",
    "        #             \"system\",\n",
    "        #             \"\"\"You are an AI programming assistant. When asked for your name, you must respond with 'SeanPilot'. Follow the user's requirements carefully & to the letter. For questions not related to software development, simply give a reminder that you are an AI programming assistant. Keep your answers short and impersonal.\n",
    "        # You should help the user to the best of your ability. If there's a tool you need, feel free to call it and use it.\n",
    "        # Start by breaking down your users' needs as finely as possible, and choose a tool to address each need.\n",
    "        # Answer in Korean.\"\"\",\n",
    "        #         ),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You're an AI programming assistant. You need to help users to the best of your ability.\n",
    "Your task is to figure out the overall structure of the project. Especially with complex projects, you'll want to keep track of how each file is related to the others.\n",
    "You should follow these steps\n",
    "1. Categorize and plan your users' needs in as much detail as possible. If necessary, choose a tool to address each need.\n",
    "2. process the plan sequentially.\n",
    "\n",
    "Answer in Korean.\n",
    "\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_directory_tree_str(root_dir):\n",
    "    directory_structure = \"<directory_structure>\\n\"\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        level = root.replace(root_dir, \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * (level)\n",
    "        directory_structure += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "        sub_indent = \" \" * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            directory_structure += f\"{sub_indent}{f}\\n\"\n",
    "    directory_structure += \"</directory_structure>\"\n",
    "    return directory_structure\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_directory_structure(root_dir: str) -> str:\n",
    "    \"\"\"Get the directory structure of the project. If you need to check the directory structure, use this tool.\"\"\"\n",
    "    return get_directory_tree_str(root_dir)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_file_contents(file_path: str) -> str:\n",
    "    \"\"\"Get the contents of a file. If you need to check the contents of each file, use this tool\"\"\"\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        # 파이썬 파일인 경우에만 코드 블록으로 감싸기\n",
    "        if file_path.endswith((\".py\", \".ipynb\")):\n",
    "            return f\"```python\\n{content}\\n```\"\n",
    "    return content\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "tools = [get_directory_structure, get_file_contents]\n",
    "\n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAERCAIAAAAyuEahAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdAFNfaxs/2vvRelI4gqHTEXqJiL1Gxi96YxJKYGDXol5hrud4keokae0RNLNgrFhCxxY6KiiIIiHTYBtvL7PfH5HKJAlJ2d2Z3zu8vdnZ2zrPLs2fOnvOe9yXp9XoAgRAGMtYCIBCTAh0PIRbQ8RBiAR0PIRbQ8RBiAR0PIRZUrAXghZpSlaxOK6/TadSISoFgLadVMFhkCo3E4VHZPIpTJybWcswDEsHn44ueSQufyoqeyzwD2CoFwuZTbJzoWpV5fCZ0FllUqZbVaylU0ptcuVdXjndXjl8YD2tduIa4jn+dI/3zrMDFi+nqw/IK5rC4FKwVdQiNGil6JivOlZW8lPccaR8UzcdaEU4houPVKiT990oSmdRzpJ21Ax1rOQZGIdX9eba2tkz10QxnG0dLe3cdh3COryhSnNlePm6hu4M7A2stRkRSqzm7szwm3s63OxdrLfiCWI4XVqkzD1dP+MIdayEm4sLeiq49rTz82VgLwREEcnzRc9nDDOGELzywFmJS0vZUuPuxQntbYy0ELxBlPr5epLl2rIZodgcAxCe6FDyWlr1WYC0ELxDF8VcOV09ZTji7o4xb6P4wXaSUabEWggsI4fh7l4QunZl0hnnPP3YEvzDuzdMCrFXgAst3vFaDPMwQRQ+zw1oIlnSJ4le+UYqq1FgLwR7Ld3x2pqjvBAesVWBPn7EOOTclWKvAHst3fO6deg9/lmna0ul0jx8/bvfLpVLpy5cvDarof3gGsp/elBBnaq45LNzxtWUqBpvMs6GZprnVq1evW7eu3S+fPHny6dOnDarob3h15RQ9kxnv+maBhTv+7St5QLjpIqtUKlX7Xoh2vWq1ccfZvt055YVEn6a0cMfXlqnZfKNM0dy8eXPSpElxcXEff/xxamoqAGDVqlXp6emFhYURERERERHl5eUAgDNnzkybNi0mJmbAgAErVqwQiUToyzMyMiIiIrKysubMmRMTE7N9+/YRI0YIhcKjR49GRESMGDHCGJp51rSqknZ+Jy0GC4+Pl9VpOXzDv0e5XL5s2TJvb++VK1cWFBTU1NQAABITE6uqqsrKyv75z38CAOzt7QEAT58+7dy5c3x8vFAoPHz4sEwmS05ObrjOv//97/nz53/22Weenp59+/ZdsGBBeHj41KlT6XSjRICx+RR5nc4YVzYjoOPbg1AoVKlUAwYMGDZsWMNBT09Pa2trgUDQvXv3hoNJSUkkEgn9m0ql7tmzR6VSMRh/BbFNmjSpoTt3dHSkUqn29vaNX25YOFZUmYTo61AW7ngqnUw2wlt0c3MLDQ397bffWCzWuHHjWuiSNRrN4cOH09LSKisrmUwmgiAikcjZ2Rl9NioqyvDimodMITHYZL1e3/AlJCAWPo6n0UkyseHv4yQSadOmTSNGjEhOTh43blx2dnaTp+n1+i+//HLPnj2jRo3asmVLfHw8AABB/relkM02aVSjTKIlk0lEtrvlO57Dp8rqjHIf53K5y5cvP378OJfL/eqrr+RyOXq88YR3dnb2vXv3li9fPmXKlK5du/r6+n7wskadL5fX6Yz0O96MsHDH27nS1UqjbNNGJyLd3NwmT54slUrRmRkWiyUQCBp6cbFYDAAIDAxs/LBxH/8OLBartrbWGGpRFDKdc2eibwC38HG8qzfr9jlB155Whr2sRqMZP3784MGDfXx8jh49yuVy3d3dAQBhYWFnzpxZt25d9+7d+Xx+SEgInU7fsmXL2LFj8/PzU1JSAAAFBQXoye/To0ePixcv7t27l8/nh4aGtuae0CbyH9V3CuQY9ppmh4X38a7eLEGFWqUw8FBeoVBERkZeuHBh/fr1NBotOTmZyWQCAOLj4ydOnJienr558+acnBxHR8e1a9e+fPly6dKld+/e3bFjR69evQ4fPtzcZRctWhQREbF79+6UlJS3b98aVjMAoPi5vHMw0fdDWf4eqFtnap06MXy7ET2nRUWx4vmfdYOmOGEtBGMsfFQDAAjpZXVyS1kLjs/MzETXjN6BwWA0FzWQkpLi5eVlUJnvIpVKm1t5tbGxaVi7bcyGDRvCw8Obu+Cdc8KoobYG1WiWWH4fDwC4erTawZXRNa7p0bxCoWjSQGq1urmJdnS1yNAy/waCIJWVlU0+pdFoaLQmYuPs7Owa1rbe4c0L2ZPrklHzXA0t0/wghOOVcu2l/VWjP3XDWghmXP69MnygjZ2rJScsaSUW/ssVhcmmhg+0OflrGdZCsOHKoSp3fza0OwohHA8AcPdje3XlpB+owlqIqbl9rpbGJMOkfA0QYlTTQOFTaWGObNBUosxX3EkTMLmU7n1gspr/QZQ+HsU7hOvszTya/FarMY982R0hbU8FiQSg3d+BWH08SmWxMutYdecgTky8ZSY4eJwlfnhF1O9jB59QmHTyXYjoeACAHtE/yBDdvyyMGmLr4c+2jHIDgnJVca7s8TWJfzi353A7Co1YN/BWQlDHo+i0+ic3xAWPpfVCbZdoHhprybejmctHQiGTJEK1TKJDEH3BYymNQfYO4YT2smLzLH9hsd0Q2vENyOu1ZQWKOoEWDS2uFxk4wLiqqkqtVnt4GDgNIN+GhiB6jhWFa0119Wbx7UyUssGsgY43BYcOHSorK1uyZAnWQiAEm6uBQKDjIcQCOt4UsFgsKysD70qBtA/oeFOgUCgkEpjlFBdAx5sCKpXaXBwvxMRAx5sCrVbb7pSUEMMCHW8K6HQ6i2WijN6QloGONwVqtVqhIHpSX5wAHW8KWCyWjY0N1iogADreRDS3lRZieqDjIcQCOt4UwNlJ/AAdbwrg7CR+gI43BTQaDU3TB8Ec6HhToNFolEol1iogADoeQjig400Bg8Hg82HGGFwAHW8KVCpVXV0d1iogADoeQjig400Bi8WytoaZknABdLwpUCgUaBEoCOZAx0OIBXS8KYCjGvwAHW8K4KgGP0DHQ4gFdLwpgNk78AN0vCmA2TvwA3Q8hFhAx5sCuCMEP0DHmwK4IwQ/QMebAiaTCWMncQJ0vClQKpUwdhInQMdDiAV0vCmg0WgwCx9OgI43BRqNBmbhwwnQ8aYARpLhB+h4UwAjyfADdLwpgH08foCONwWwj8cP0PGmgE6nczgcrFVAAKxgbFxGjx6t1+sRBFEoFDqdjs/nIwgCADh37hzW0ogLFWsBloyfn19WVlbDQ6lUCgCIiIjAVBTRgaMaI5KYmGhra9v4iJWV1ZQpU7BTBIGONyZBQUGhoaGNj3h7e/fp0wc7RRDoeCMze/bshgpQsIPHA9DxxiU4OLhHjx7o315eXv3798daEdGBjjc6M2bMsLGxsbKymjZtGtZaIHCu5r+oVTpBuUYh0xn8ymzgFRM6WiQSedpHFj6TGfz6DCbZzpXOZFMMfmWLBM7HAwDAlcPVBY+lDu4MKs38bnoUGqksX+4ZyB4yw4lEImEtB+9Ax4PT28vd/Tn+4eadT6Y0X/b4qmDCIncaw/y+tKaE6I4//1uFewDHO8QSNqEKKlS3z1YlfOOJtRBcQ+j+4G2+nEIlW4bdAQB2LgxXb/bLB3BDbUsQ2vHCcrWFjQGYXGr1W5gmpCUs6v/dVuRSHd+BjrUKQ8K3o6kUhB6mfhBCO16r0SNai/IHogNqueEnWC0JQjseQkCg4yHEAjoeQiyg4yHEAjoeQiyg4yHEAjoeQiyg4yHEAjoeQiyg4yHEAjoeQiyg4/GITqd7+vQx1iosE+h4PPLThtUbk9dhrcIygY43CmXlpR3ZXKaGpTCNBsxl0DYuXDxz6tSRwqICFosdFRm7YP4Sa2sbtO7NnpRtGVcuKBTy0NCwV69eTJ82d/SoCQCAisryrVs3Psy+S6cz/P0CExM/DwwIAgCMHN3vyy++vXnz6p27Nzkc7sgR42fO+AcAYP2Pq65mpQMA+g+MAAAcPnjOyckZ6/dtOUDHt43c3Keenp0HD44XiYQnTh6WyWX/WpsMANi+85czZ47NnTPf3t5x2/b/qFTKYUNHAQAEgtqFixLd3DwWzF9CIpEuXz7/xZdzt2/93cvLBwCw/t/fz5o5b/LkmVlZ6Xv37Qjw7xIT02valMSa6qqKirJvl/8TAGBjY9sKXZDWAh3fNr5anNSQIYNKpf5xYI9KpaJSqefOnRgeP2bSxOkAAL1ev3bdyqfPHoeHRf3+x24ba9sNP22jUqkAgMGD4qfNGHMu7eTC+UsAAPHDRk+dMhsA4Ovjfz7t1L0Ht2Nierm7e1pZWQtFgpCQ7li/XQsEOr5taDSaEycPp2ekVVdXMhhMBEHEYhGNRlOr1W5uHug56B/19XUAgLt3b1XXVMWP6N34CjXVVejfTOZfJS8pFIqDg6OgtgaL90QsoOPbgF6vT1rxZd6r3JkzPgkKCr1xI/Nw6n5Ej1hZWXM53KdPH388YSoA4MWLZwAAH28/AIBQJIiN7f3J3IWNr8PhcN+/OJVC1SFww57RgY5vA0+eZD/Mvrciac2ggUMBAGWlJehxCoWSkDBr1+4ta9ausLd3PH3m6PhxCR4enQAAPB5fIhF7enZua1sEzyNkPODsZBuQ1IkBAP5+gY0fooVuxoyeGBkRIxIJpdL6FUlrFsz/Gj0nLCzq2bMnea9eNFykNaWMmUyWUChArwwxLLCPbwNBXULodPqu3VuGDx9bWJh/8FAKAKCosMDN1X312iQ+3yo2tg8AgARIVVWV6JTizBmf3Llz85ul8yd+PM3GxvbevT91iG7NPze03FC30LALF89s/M+6kK7draxtYqLjTPUWLR/o+Dbg4OC4csXaX7duWPXD0uCg0I0bdqTs3X7i5OFevfqF9Yjcu2/HlcxL6JkUCmXpku8++mi4m6v7lk17tu1IPnBwD4lE8vMLHDtm0gcbGjw4Pu9V7uX087fv3Bg5Yjx0vAEhdN7JG6dq6SxqUIwBagvrdDoK5a981nX1dcu/XUSlUjcl7+74ldtEyUtZ8dO64XNdTNyuGQH7eMOwYePa169fxcb2sba2KXlbXFiYP3z4WKxFQZoAOt4wREX1rK6uPH7ioEajcXFxmzH9H+hMJQRvQMcbhn59B/XrOwhrFZAPA2cnIcQCOh5CLKDjIcQCOh5CLKDjIcQCOh5CLKDjIcQCOh5CLKDjIcQCOh5CLAgdZcDmUixszwWJBHi2hP6ffhBC9/E8W2r1GyXWKgxJ9Vslm0fBWgWuIbTjPfxZCqkWaxWGpF6o9gxkY60C1xDa8SwutVsfqysHyrEWYhhunKx082E6ejCxFoJrCL0HCqU4V379RHVwTxs7FyaTY35DAo0aqS1VvnlR7xPKDYmzwloO3oG/csDeIz9OnzyvJEdZ/Ky+XmSUQY5Go0EQhMFgGOPi1o50rhVFRntSJNCGgFHGaMKSIHoff/78eQRBRo4cadRWEhIS9Hr9pk2bHB0djdfKDz/8sHjxYj6fb7wmLADijuPPnz8PAOjfv7+x7X7q1Kny8vLi4uJTp04ZtaHvv/+eTqffv3+/srLSqA2ZNQR1/MmTJ588eQIAYLONPrNx9OhRmUym1WovX75cXV1t1LaYTGZISMicOXOM3ZD5QjjHo1bw8fFJSkoyQXPHjh0rLS1F/y4uLj527JixW2QymefPnxeJRPX19cZuyxwhluOvXr2anJwMAAgNDTVNi8eOHZPJZA0P09PTa2pMkUA4ICCASqVOmDBBqbSoJbaOQyzH5+XlrVtnuvpKJ06cKCkpaXyktLT0yJEjpmmdxWL99NNPx48fh6ZvDCEcX11d/c033wAAPv30U1O2+8cffyiVSv1/QRAEQZD09HSTCfDy8po6dapcLt+w4QOZLokDIWYnv//++6+++srKCrPVmUOHDpWVlS1ZsgQrAQcPHnR3d+/Tpw9WAvCDhffx6C/FH374AUO7o/VzjLT81EqmTJnSrVs3tVr94MEDDGXgAYt1vF6vHzNmjI+PD9ZCAABArVZrNBpsNVhZWdHp9F27dl29ehVbJdhimY6XSCQqlWrz5s09evTAWgu+2LFjh1qtxloFllig47du3SoUCplMpoeHB9Za/oJCoXC5TdR+woQhQ4YAADZs2CAWi7HWggGW5vi8vDwGg+Hl5YW1kL8hlUp1OnxVNZs3b97XX3+NtQoMsBzHi8Xily9fOjs7z5kzB2st74IgCIvFwlrF3+Byub/99hsA4NatW1hrMSkW4nixWDx+/HhfX19s52SaQywW483xDUil0o0bN2KtwnRYQny8QqF4/fr1lStXsBbSLBKJBJ9fRXRY31DPhwiYfR//yy+/aLXa8PBwrIW0BJ4dDwAYNGgQAGD79u1yuRxrLUbHvB1/69YtGxsbHo+HtZAPoNVq7ezssFbxAWbNmoVO41g2ZhxloNFoKioqPD09sRbyYfr163f27Fn8fzPRooXV1dUuLhZbLdBc+/hRo0ZRqVSzsLtUKtXr9WZhd3TpIDc3Ny0tDWshxsIsHZ+amrpt2zYSiYS1kFZRWVkZGxuLtYo2MHDgwNu3b5vvzb9lzMzxSqVSLBaPGzfOzc0Nay2tJS8vj06nY62ibaxevVqv11tk2Jk5OV6r1Q4cONDa2ppGo2GtpQ28ePGiS5cuWKtoM2QyWafT/f7771gLMTDm5PjMzExzXCDMy8sLCAjAWkV7iI6OJpPNySGtoW0rUDqdDpPIO71eX1xc3Lt3b4VCQafTzWvFhE6nm2MfjzJ16lShUIggiL29PdZaDEPbHK/RaDDZIS8QCOzt7dGm+Xy+GTk+JydHLpfjNsSgNdja2m7dupXBYOAwYKkdmEGUgU6nw//yTXPcvXs3OjoaaxUd5fPPPy8pKamsrHR2dsZaS0fB+yhNq9Wa9VDyzp07MTExWKswAJ6ennK5vLa2FmshHQXXZpJKpVqt1lzm3d9HqVQymczu3btjLcQweHt7r1y58v79+1gL6RAGcHxVVVUHEx1KJJL4+Hg0EWQDCIKw2Wwm04yzoV+4cMHCluu3b99OpVLNOgFORx1fUVGRmJiYn59vID1/gSZ4MevxDAAgLS0tPj4eaxUGpkePHmadybWjltJqtcZYjhYKheZu98rKyvLy8rCwMKyFGB6pVDpz5kysVbSTtsVOKpXKurq6hoeVlZWJiYkNDwcNGvTVV1+hft21a9eDBw90Ol1QUNCcOXMaNp5euXLlyJEjFRUVtra2Q4cOnThxIplMlkgkCQkJ8+fPHz58OACgqKhox44deXl5PB4vMjJy/vz5jd3P5/PNYqiTmpqqUChmzZqFtRCjUFBQUFdXZ47f5w7NTtra2i5duvTHH3+cPn16aGiotbU1+q349ttv6+rqEhMTGQzG0aNHk5KSdu3axeVyMzIyNm7c2K9fvxkzZrx8+XL//v0AgMmTJ79z2a1bt5aWls6bN08ul+fk5JhpZ79z587jx49jrcJY+Pr6Yi2hnXTI8XQ6HU2B5O7uHhwcjB68evXq27dv161bh85RBAcHJyYmnjlzJiEhYd++fcHBwUuXLgUAxMXFSaXSo0ePjh49uvE1xWJxVVWVj4/P0KFDAQDjxo3r2BvEhtOnT/ft2xftAiyYvn37Xrp0ySxuuQ0YvvvMycnhcDgNU3JOTk4eHh6vXr0qKysTCARxcXENZ4aFhSkUirKysoYjSqWSxWINGDAgOzt727ZtIpHI4PJMw549exqP9yyVvXv3pqamYq2ibRh+zVUul7+zp5PH4wmFQjSNeuNuD90kUVtb6+DggB5Be4uZM2daW1unpqZevnw5MTHR2FVrDM6VK1cCAgLc3d2xFmJ0vLy88JYa6IMYvo+3s7N7J/ZGJBJxOBzU1hKJpOE4mhOrYXMQgiBoGiMSiTRmzJjffvstJiZm27Ztz58/N7hIo7Jjx46FCxdircJ0LFq0SKs1m0LQHXU8mjJXIBA0HOnSpUt9ff3Lly/Rh0VFReXl5cHBwba2tk5OTo03Gdy4cYPBYHh7e6Px7rW1tWiImEqlQis0TZ8+HZ0W6KBIU3Lo0KGoqCj8JAA0AYMGDVq7di3WKloLZdWqVa0/W6vVonZsgM1mZ2Zm5ubmslisR48e+fr6ent7X79+/dq1aywW6/Xr17/++iuVSl28eDGLxeJyuSdOnKitrdVoNGfOnLl69erkyZPDwsJoNFpmZubz58+5XK6fn9+aNWvu3bunUCjS0tJKSkoSEhIahj3od4xKxWkAnFar/eSTT1JSUrAWYlICAgKioqLIZLJZzKp11PEkEikwMPDhw4fXrl2rqqqKjY3l8/nR0dHFxcXnz59/8OCBr6/v8uXLnZyc0MAMa2vra9eupaenSySSiRMnTpo0CQ2bCQwMfPXqVVFR0ZAhQyoqKu7fv3/t2jWlUpmYmPjOJlE8O37dunXjxo0z32j4doMgSF1dnQkKJ3acDq1AGRCpVNrK7Lu4XYHKz8/fu3evGd3fDcvs2bMXL15sspJy7QYXtyG1Wo0gCNYqOsrixYsXLFiAtQrMWLZsmVnsBMdFH6/T6UgkUitHgfjs4zdu3Ojk5DR16lSshUA+AC76eAqFYhY/eprj0aNHubm50O7Pnj3LysrCWsUHwIXP0KxdWKtoP8nJyT///DPWKrAnICBg+fLlWKv4ALhwvFKpNN+NTsuWLZs+fbrFh9C0BhqNduDAgcaLMzgEF9N8fD4fawnt5NChQw4ODmg2aggAACfFFVugbY6n0+mYd2b4mYxHM5JaXtauDjJ37lx0cyDWQpqmbbLIZLLBUyhWVlaePHnys88+M+xlTcDs2bPNMUeasbGzs8vKysLtfQ/7cbxCocBzQZvm+Pzzz3fu3InbngxDVqxY0bVrV6xVNAv2FROUSuWTJ0/MK43R6tWrQ0JCxowZg7UQSJvBvo9nMpnmZfeDBw+y2Wxo9xaYN29edXU11iqaBnvHAwC+++47c0mBcvfu3Zs3bxKz9m/rsbOze/ToEdYqmgYXw9DXr18XFxcHBgZiLeQDVFVV7dq1a/fu3VgLwTtowQWsVTQN9uN4AMDDhw9dXV1xnr5Lq9XGxcXdvXsXayFmgFarlclk+CzoiYtRTXh4OM7tDgAYNmzYhQsXsFZhHkilUtwmocCF4wsKCrZu3Yq1ipaYOnXq5s2bbW1tsRZiHlhbW+v1+sZ7mvEDLkY1er0+MjISt9HVixYtSkhIMK96fZij0WioVCoOw6Vw4XgAQFlZmZ2dHQ4D35OSkvr27UuEYtYEAS+Oxyfr16/38fH5+OOPsRZifmzdutXBwQGHHx0uxvFodA3edoj+8ccfXl5eOPyfmQU8Hq9xtjn8gIv5eACAs7NzdnZ2cXFx586dsdYCAAC7d+/WaDTmGN+GE6ZNm4bP4QOORjX19fUkEqmVGQ2Myr59+yQSyaJFi7AWYsYoFAqZTIbDmph4GdWgATZSqRRrFeDUqVNarRbavYPk5OR89913WKtoAhw5nkajffHFFwUFBR999FFYWFhCQoLpNaSmpr569coyCpdiC5PJxGesFI5GNQMHDkTTZ6N5DaKioky8LHX06NHXr1/jf2+yWaDT6eRyeUMaXfyAi1+uw4cPr6ioaJy4UK/Xu7m5mVLD2bNnKyoqoN0NBYVCwaHd8TKqSU1NfWdHMIlEMuWkzYkTJ3JycuDY3YC8efMGnzNduHA8l8tNSUlpnHufxWKZLLbs5MmTubm5K1asME1zBIFMJms0GqxVNAEuHI+afv/+/Q1p1zkcjqOjownaPX369NOnT1euXGmCtgiFh4cHPjcS4MXxaL9+5MgRtGun0+nOzs7GbvH06dNPnjzB5ySauaPVavFZ6LhVczVaDaKQmij3L1p0gMFgbNu2zagNZWRkPH/+/Isvvmjfy/V6wLfFxe9+fFJcXPz111/jsL7nBxz/4l5dzg2JsFLN4lJMpkmv1xs7yhRBEL1ejxbhaR92royyfLlvd27PkXZsHrT+X8yePfvJkyfv/Pv0en12djZ2ov5GS/+qe5eFteWa3uOcebY0E0oyGzRqRFSlOvCvkklLPPjwIwIAzWKQlJT0Tsp1XKXma3Ycf/eiUFKj7T3WCdq9OWh0sqMHa/Iy78M/v1XIdFjLwQUxMTH+/v6NBw4MBmPChAmYivobTTteVK2uLVPFjDDFbIkF0H+yy59na7FWgRdmzpzZeE+3m5vb+PHjMVX0N5p2fG2ZSq/H3X4t3GLtQC98KsNaBV6IjY1tSMRCoVDGjBnTkd9LBqdpx0slOgcP3G3Awy1MNsXRgyWTmE0VX2MzY8YMNMTA3d190qRJWMv5G007XqNCNEqzL0VmSgTlZlz0weDExMQEBQWRyeTx48fjqoPHSyQZBFvUSqQ0Xy6VaOV1OgTRy+sN8Cu8T5f5XEW2AxiQcaiq41ej0ckcPoXNo/LtqO5+HaoaCx1PaHJuiF89ktWUKh29eDqNnkKnUug0vd4As3N0lltMnJtUYQiVAOjrkZoqrU6jplBJNbsqvII5/mFc75D2bJeDjicoj66Kbp0VuPhZMe2suvgZPaDDgNh42tbXyJ/cUtw+L+w9xt4zsG1dPnQ84agpVV36o5rOZQQP6myOvz0oVLK1CxcAwLRRXT8tsr9XP3SGU+tfjqNIMogJeHGv7tyeKpcgJ0cfO3O0e2NYfIZ7qLOWwt2ZVCira+1EGXQ8gSh8KntyS+YV6Uah4Wv+pCNwbZne0e4H1peolK36wQ0dTxSeXBffvSxxDrTAdXQqneLfu9O+H97I6z/c00PHE4LyQkXOzXqXoDaMd80O72j3A+vffvA06HjLR61EbpwSevRwxVqIcaEyKG7B9pcPfKD+FHS85XP9ZC2N26FVG3OBbcOufKN++0rewjnQ8RZOvUhT9Exm68HHWoiJsPe2vXFK0MIJhnR87otnKpWqI1fIupbRf2BESUmx4UQ1y5p1K2fMwlEUq5HIvip29LPDWkUT1AreLvm/6Ec5lw17WbYVg86hF+U2m87RYI6/eOmvcvwbAAAKV0lEQVTs/AWzlEoDLStDDMSLu/VcWxbWKkwKmc4oeNRs8LbBHN/B3h1iDMoKFBxrOoVGrLEr35Fd9KxZxxsmyuDipbPJv6wHAIwZNwgAsGzp90OHjAQAXL58/sChlPLyUjs7++HxY6dOmY3m2dNqtSl7t1+6fE4iEXfq5DVr5rxecf1a31x+Qd7CRYnr123auXvz69evnJxc5v1jUVxcX/TZ5hoFAGRevbxv/86qqorOnbwR5H/h0Eqlcvdvv17JvKhWqzzcO02cOH1A/48M8slgS2mBnOvAMdLFCwofpqVvLa98xePa+npFDBv8GZ9nDwBYuXbg+JHLnr3Iys27xWJyYyLHftR/LvoSqUx0Ou0/z19ep1EZPl7hRhJGpVNsXNgVRQoXryZubob59kdHxU38eBoA4F9rkzcl746OigMAXLp07l///t7PL/D/Vq7r13fwnpRtBw6moOf/vGFN6pHfRwwfuyJpjbOz6/99tyQnp20lnlUq1Q+rl08YPyV5405nJ5c161ZIJOKWG824cnH1miQ7W/uFC76JjIx9XZiPHkcQZMXKxbdvX586ZfbiL5N8fQNWr0lKu3DaIJ8MtlSVqMlUoyyv5r++v2v/IidHr4ljVvTpOaWw+NH2lPlq9V/JhA+f+MHV2f/zOdvDug27nLkrN+8WAECjVe/Yu/D5i2t9ek4ZPmSBUFRuDGEoapW+XtT0apRh+ngbG1tXV3cAQJcuXa2srNGEDbv3/BoS0n1l0hoAQJ/eA+rr6w6n7hs/LqG2tvrS5XMzps+dNXMeAKBvn4HTZozdu2/Hxg3b29TowgXfoD3x3LkL5n067UlOdu9e/ZtrlEKhbPn159DQHj/9+Cu6R6Gs7G3B61cAgOs3MnOePjp04Ky9vQMAYNDAoQqF/PiJQ/HDRhvkw8EQeb2O52IUx586vyEmYuzYEUvQh/6+0T9tmpRXcCckqB8AICps1MC+swAArs7+9x6eflVwJygg7tadoxWV+Z/M3OzvGwUA6OwR8uMmY22PotApzW1JM1bsZGlpSW1tzaSJ0xuOREbGpl04XVpWkpeXCwDo1as/epxEIkVGxKRnpLW1CRbzr3uWk5MLAKC2tqaFRuvqJBKJeML4KQ1bcsj//ePOnZtarXbKtFENr9LpdBwO9qVKOo5KrrNlGN7xQlFFVU1RrfDtnQenGh8XS/7a/EGn//WvoVAoVnxHSV0NAODZi2suTr6o3QEAZLIRY3sodEpzsWXGcrxUJgUAWFv/r+Qvj8cHANTWVMtkUgCATaOn+HwruVwuk7VzczSNSgMAIIiuhUbFEhEAwNm5iXVHkUhgZ2e/8ee/3WEoVEuIo9YjRqkOUC8VAAAG958bGtS/8XEer4kaOGQyFUF0AACxpNLNJcAIcpqmudQEBv6/NiQqcXRwAgCgY2sUkUiIWtDe3hEAUFcnQUcRAAChUEClUjtezLWFRtGHYrHo/VfxeHyxWOTk5MJgMDooAG+weFStSsdgGzjjEIvJAwBoNCpHhzZkPOdybKSyJj5/Y6BT63jW9CafMti8FTrGqK2tQR/a2dk7O7ncu3er4YRr1zKYTKavb0CXLl1JJNKduzfR42q1+s7dm8HBoRQKhU6jo1+G9mlooVEfH38ymZxx5cL7rwoLi9LpdGfOHms4olBYyKoCh0/Rqg2fOsrB3tPayvl+9lmV+q8PSqfTarUfyJ3t5hLwtiy3uuaNwfW8D6LRsflNj5oM1scHd+1GoVC2bP152JBRKrVq1Mjxs2bOW//jqp9+Xh0ZGZudfe/mrayZMz5hsVhuLPchH43Yu2+HTqdzdXU/f/6kUChI+nY1AMDL25dMJv/nl38tmL+kR/eIdshorlEWizVs6KjzaafUKlVUVE+BoPbu3Zs2NnYAgMGD4s+eO7F9xy8VleX+foEFBa9u3rq6d88xHBYQbyuOnoyyEsM7nkQijY5fvO/Qss075sRGjUMQ3YNHaeHdh/bp2VLprv69Zzx4nLZ1z6d9YifzefbZOZcMLqwBKg1Y2Td9Z6OsWrXq/aNlrxU6LXDu3Ia1Oj6P7+DglJWVfvv2jfr6uiFDRvj6+tvY2GZevXzh4hmxSDhlyuxpUxPRfTeREbEymfTCxdOZmZc4bM6Sr1dGRsYCAHhcnouza/aj+2QSOTIiprm2hELB2XMnBg4Y6uHRCQCg0WgOHkqJiowNCgppodHw8GiZTHrrz2v37/9JIpF4PL5CoRg7ZhKFQunXd7BUWpeVlX79RqZMLh02dHRISPeGWfzWkHtb3LWnFY2Br7UeMgW8uC22cjF8dRonh87ubkGFxY8fPk4rKX3u4uIb3n0YOh+feWO/u2tggG80euad+6eYTE6P0I/YbL6XZ7eS0mePn2WUV+b7dg4vfpsTGjzAxcnAWSk1Sm11gajPWIcmn206t/C9S0K1EnTrZ9vUSyBNcHRD0eQlns3dSTFkZ1Khd7Q7lY47YcZD8LbOxkozYFLTe1/wOyOx6Mu5RUUF7x/v2bPvt8t+wEKRWRIUY1Vbq0C3QjdJRtaerFsH3j/u7hJYWvGyyZcs/MduJ0evJp9qB2npW/+810SaeRaTp1DWN/mSL+alONh7NndBRK3x69Hs+8Wv479b+S9NUz+GGqbhIa0hfKD1/tVvWnB8XPTHYd2Gvn+cRGq2toAV35BbB/vGTY2JGPP+cb0eNLf1vAUBMpESaDUe/s3uB8Cv4xvmLiEdgcWhdInmVb8R23eybvoEFo/FwrIMJYdtxWFbteLEVlFTKBjWYjIPfP3SghiDPmMdtDILmW9tGalA3jmQ7eTZ0roKdDwhGDzFofh+KdYqjItKphYUCftNaGLdtzHQ8YTAzoURE2/z9gkei+8Zivw/y6Z92+zP2Qag44lCYAR/4ETb0pwKrIUYHqVU/Tyj6LMfvSnUD2dZg44nEK7erF4jrPNvlqhkaqy1GIz6Wll1XvWnP/pQqK0yM3Q8segczE34xqO+XFj5skarMu9qbVKBovhBGZ+jmrGyE4XS2hya+J2dhBgJrjV1wiK3F/fqbp4u4zuymHw2z5FNJptN1lW1QlNfLddrNSS9Nn62o6N728KfoOMJSpcofpcoft7DuvxHsheZ1Q6duVq1nkKj0Fh0PWKMoPoOgegQRKPVqnQUGkkuVnl35fj14Ln5tiftFHQ8oQkI5weE89HYQZlEK6vTajWIUoq7EmA0BonDZ3KsKHw7qr1rh2JaoeMhAADg5kOU2I2mHU9nkhBgNgM7PGDvxgQk3A0GIO/T9FwNz4ZW84YQ69IGQSHV1pQq2Tx4wzQDmna8owfDzCummBRRldqnmyXkPiACzfbxbr7M68cteVHagGQcKO895gPhHBCc0GwMNADg+W1J/mNpt752Nk70Vi5oEQqZRCOuUV85UJG4ujOLA4c05kFLjgcAFD2XPb4mrixStiZigVA4ejJEVRrvUE7vMfZmtHwD+YDjG1ApcDdHiy16vZ7JJtDmUYuhtY6HQCwDODqHEAvoeAixgI6HEAvoeAixgI6HEAvoeAix+H+cv+fS4iUVqAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "def summary_agent(state: State) -> State:\n",
    "    messages = state[\"messages\"]\n",
    "    print(\"==================== [MESSAGES] ====================\")\n",
    "    print(messages)\n",
    "    response = chain.invoke({\"messages\": messages})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "work_flow = StateGraph(State)\n",
    "work_flow.add_node(\"agent\", summary_agent)\n",
    "work_flow.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "work_flow.add_edge(START, \"agent\")\n",
    "work_flow.add_edge(\"tool_node\", \"agent\")\n",
    "work_flow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,  #! 변경 (return Literal[\"tools\", \"__end__\"])\n",
    "    {\"tools\": \"tool_node\", \"__end__\": END},\n",
    ")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = work_flow.compile(checkpointer=memory)\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ messages ]\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "main.py 파일을 자세하게 설명해주세요.\n",
      "None\n",
      "==================== [MESSAGES] ====================\n",
      "[HumanMessage(content='전체 프로젝트 로직에 대해 설명해주세요.', additional_kwargs={}, response_metadata={}, id='49be0159-ec68-47f2-b7c7-65513c6e32a0'), AIMessage(content='프로젝트의 전체 로직을 이해하기 위해서는 먼저 프로젝트의 디렉토리 구조를 파악하고, 각 파일의 내용을 확인하여 파일 간의 관계를 분석해야 합니다. 이를 통해 프로젝트의 전반적인 구조와 흐름을 이해할 수 있습니다. 다음은 이를 위한 단계별 계획입니다.\\n\\n1. **디렉토리 구조 파악**: 프로젝트의 루트 디렉토리에서 시작하여 전체 디렉토리 구조를 확인합니다. 이를 통해 프로젝트가 어떤 파일과 폴더로 구성되어 있는지, 각 파일이 어디에 위치하는지를 알 수 있습니다.\\n\\n2. **파일 내용 확인**: 각 파일의 내용을 확인하여 파일이 어떤 역할을 하는지, 어떤 기능을 구현하고 있는지를 파악합니다. 특히, 주요 파일(예: `main` 파일, 설정 파일, 주요 모듈 파일 등)의 내용을 중점적으로 확인합니다.\\n\\n3. **파일 간 관계 분석**: 파일 간의 의존성이나 호출 관계를 분석합니다. 이를 통해 각 파일이 어떻게 상호작용하는지, 어떤 파일이 다른 파일을 호출하거나 참조하는지를 이해할 수 있습니다.\\n\\n4. **전체 로직 설명**: 위의 정보를 바탕으로 프로젝트의 전체 로직을 설명합니다. 프로젝트의 주요 기능, 데이터 흐름, 모듈 간의 상호작용 등을 종합적으로 설명합니다.\\n\\n이제 첫 번째 단계로 프로젝트의 디렉토리 구조를 확인하겠습니다. 이를 위해 `get_directory_structure` 도구를 사용하겠습니다. 프로젝트의 루트 디렉토리 경로를 알려주시면, 디렉토리 구조를 확인하겠습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 348, 'prompt_tokens': 204, 'total_tokens': 552, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None}, id='run-35ec8929-cda1-4902-8ec0-26e2a5e57147-0', usage_metadata={'input_tokens': 204, 'output_tokens': 348, 'total_tokens': 552, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='네, /home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt 입니다.', additional_kwargs={}, response_metadata={}, id='c15802ea-12dd-4667-bb14-fb647053bc2c'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZS4ybx3BQjwA7toSWrf58rXN', 'function': {'arguments': '{\"root_dir\":\"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt\"}', 'name': 'get_directory_structure'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 576, 'total_tokens': 606, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f3cbf7c3-95e7-4758-8399-937a26b5ae01-0', tool_calls=[{'name': 'get_directory_structure', 'args': {'root_dir': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt'}, 'id': 'call_ZS4ybx3BQjwA7toSWrf58rXN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 576, 'output_tokens': 30, 'total_tokens': 606, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='<directory_structure>\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n</directory_structure>', name='get_directory_structure', id='497db54c-5e0f-4f75-b73f-328d4766e41f', tool_call_id='call_ZS4ybx3BQjwA7toSWrf58rXN'), AIMessage(content='디렉토리 구조를 확인했습니다. 이제 각 파일과 폴더의 역할을 이해하기 위해 주요 파일의 내용을 확인하겠습니다. 이를 통해 프로젝트의 전체 로직을 파악할 수 있습니다. 주요 파일로는 `app.py`, `main.py`, `graph/` 디렉토리 내의 파일들, 그리고 `prompts/` 디렉토리 내의 YAML 파일들이 있습니다. \\n\\n다음 단계로, 이 파일들의 내용을 확인하여 프로젝트의 구조와 로직을 이해하겠습니다. 이를 위해 `get_file_contents` 도구를 사용하여 각 파일의 내용을 확인하겠습니다. 주요 파일들의 내용을 병렬로 확인하겠습니다.', additional_kwargs={'tool_calls': [{'id': 'call_P8aDhj8OcKQjORZP342KtowE', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_vi9Me8SHzlCE5SZJgL6P1tkM', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_QYXgGsKAyYHRvzhu2ZaDVvaj', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_99HdUTHi6Wbpvc7PGAY0ojdv', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_tJScz9GBDwO0oncOO5v98YK6', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_qesMqoGJSqMqA3KEf7D6YdOq', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_7KpU68z4YeZ4uRoKleOh8cAG', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/gather.yaml\"}', 'name': 'get_file_contents'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 392, 'prompt_tokens': 733, 'total_tokens': 1125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e25ad73a-ee59-4cf7-bd86-b0a36e7035ff-0', tool_calls=[{'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py'}, 'id': 'call_P8aDhj8OcKQjORZP342KtowE', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py'}, 'id': 'call_vi9Me8SHzlCE5SZJgL6P1tkM', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py'}, 'id': 'call_QYXgGsKAyYHRvzhu2ZaDVvaj', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py'}, 'id': 'call_99HdUTHi6Wbpvc7PGAY0ojdv', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py'}, 'id': 'call_tJScz9GBDwO0oncOO5v98YK6', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml'}, 'id': 'call_qesMqoGJSqMqA3KEf7D6YdOq', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/gather.yaml'}, 'id': 'call_7KpU68z4YeZ4uRoKleOh8cAG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 733, 'output_tokens': 392, 'total_tokens': 1125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='```python\\nimport streamlit as st\\nimport requests\\nimport json\\nimport time\\nimport uuid\\nfrom typing import Optional\\n\\nst.set_page_config(page_title=\"Meta Prompt Generator\", page_icon=\"🤖\", layout=\"wide\")\\n\\nst.title(\"Meta Prompt Generator 🤖\")\\nst.markdown(\"LLM과 대화하며 프롬프트를 생성해보세요.\")\\n\\n# 세션 상태 초기화\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\n# thread_id 초기화 (세션별 고유 ID)\\nif \"thread_id\" not in st.session_state:\\n    st.session_state.thread_id = str(uuid.uuid4())\\n\\n# 이전 메시지 표시\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# 사용자 입력\\nif prompt := st.chat_input(\"메시지를 입력하세요\"):\\n    # 사용자 메시지 표시\\n    with st.chat_message(\"human\"):\\n        st.markdown(prompt)\\n\\n    # 메시지 저장\\n    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\\n\\n    # AI 응답 생성\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n\\n        try:\\n            # SSE 스트림 연결\\n            with requests.get(\\n                \"http://localhost:8000/chat/stream\",\\n                params={\"query\": prompt, \"thread_id\": st.session_state.thread_id},\\n                headers={\"Accept\": \"text/event-stream\"},\\n                stream=True,\\n            ) as response:\\n                response.raise_for_status()\\n\\n                for line in response.iter_lines(decode_unicode=True):\\n                    if line.strip():\\n                        if line.startswith(\"data: \"):\\n                            try:\\n                                # JSON 파싱\\n                                data = json.loads(line.split(\"data: \")[-1])\\n\\n                                if \"text\" in data:\\n                                    full_response += data[\"text\"]\\n                                    message_placeholder.markdown(full_response + \"▌\")\\n                                elif \"error\" in data:\\n                                    st.error(f\"Error: {data[\\'error\\']}\")\\n                                    break\\n\\n                            except json.JSONDecodeError as e:\\n                                st.error(f\"JSON 파싱 에러: {e}\")\\n                                continue\\n\\n                            time.sleep(0.01)\\n\\n            # 최종 응답 표시\\n            message_placeholder.markdown(full_response)\\n\\n            # AI 응답 저장\\n            st.session_state.messages.append(\\n                {\"role\": \"assistant\", \"content\": full_response}\\n            )\\n\\n        except requests.exceptions.RequestException as e:\\n            st.error(f\"네트워크 에러: {str(e)}\")\\n        except Exception as e:\\n            st.error(f\"예상치 못한 에러: {str(e)}\")\\n\\n# 디버깅용 세션 정보 표시 (필요시 주석 해제)\\n# st.sidebar.write(\"Session Thread ID:\", st.session_state.thread_id)\\n\\n```', name='get_file_contents', id='8062cfb0-c500-4a94-bcc1-e9e2ee12e976', tool_call_id='call_P8aDhj8OcKQjORZP342KtowE'), ToolMessage(content='```python\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import StreamingResponse\\nfrom langchain_core.messages import HumanMessage\\nfrom graph.graph import graph\\nimport json\\n\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/chat/stream\")\\nasync def stream(query: str, thread_id: str = \"1\"):\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n    async def event_stream():\\n        try:\\n            async for chunk in graph.astream(\\n                input={\"messages\": [HumanMessage(content=query)]},\\n                config=config,\\n                stream_mode=[\"custom\"],\\n            ):\\n                yield f\"data: {json.dumps({\\'text\\': chunk[1]})}\\\\n\\\\n\"\\n        except Exception as e:\\n            print(f\"========== {e}\")\\n            yield f\"data: {json.dumps({\\'error\\': str(e)})}\\\\n\\\\n\"\\n\\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\\n\\n```', name='get_file_contents', id='0a1d97d4-8c62-4513-a308-02d3df4bbc1a', tool_call_id='call_vi9Me8SHzlCE5SZJgL6P1tkM'), ToolMessage(content='```python\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated, TypedDict\\n\\nfrom graph.nodes import information_gather, generate_prompt, add_tool_message\\nfrom graph.edges import get_state\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\n\\nmemory = MemorySaver()\\n\\nflow = StateGraph(State)\\nflow.add_node(\"information_gather\", information_gather)\\nflow.add_node(\"generate_prompt\", generate_prompt)\\nflow.add_node(\"add_tool_message\", add_tool_message)\\n\\nflow.add_edge(START, \"information_gather\")\\nflow.add_conditional_edges(\\n    \"information_gather\",\\n    get_state,\\n    {\\n        \"add_tool_message\": \"add_tool_message\",\\n        END: END,\\n    },\\n)\\nflow.add_edge(\"add_tool_message\", \"generate_prompt\")\\nflow.add_edge(\"generate_prompt\", END)\\n\\n\\ngraph = flow.compile(checkpointer=memory)\\n\\n\\ngraph.get_graph().draw_mermaid_png(output_file_path=\"./graph.png\")\\n\\n```', name='get_file_contents', id='17ab4a79-3233-4d08-b385-9a9712a3d073', tool_call_id='call_QYXgGsKAyYHRvzhu2ZaDVvaj'), ToolMessage(content='```python\\nfrom langgraph.graph import END\\n\\nfrom langchain_core.messages import AIMessage\\n\\n\\ndef get_state(state):\\n    messages = state[\"messages\"]\\n    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\\n        return \"add_tool_message\"\\n    return END\\n\\n```', name='get_file_contents', id='e0535fc2-06b4-459c-9dfa-ce3576858f7e', tool_call_id='call_99HdUTHi6Wbpvc7PGAY0ojdv'), ToolMessage(content='```python\\nimport yaml\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_core.messages import SystemMessage, AIMessage, ToolMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.types import StreamWriter\\nfrom pydantic import BaseModel\\nfrom typing import List\\n\\nload_dotenv()\\nMODEL_NAME = os.getenv(\"MODEL_NAME\")\\nTEMPERATURE = os.getenv(\"TEMPERATURE\")\\n\\n# * information gather\\n\\nwith open(\"./prompts/gather.yaml\", \"r\") as f:\\n    gather_template = yaml.safe_load(f)\\n\\ngather_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (gather_template[\"role\"], gather_template[\"template\"]),\\n        MessagesPlaceholder(variable_name=\"messages\"),\\n    ]\\n)\\n\\n\\nclass PromptInstructions(BaseModel):\\n    \"\"\"Instructions on how to prompt the LLM.\"\"\"\\n\\n    objective: str\\n    variables: List[str]\\n    constraints: List[str]\\n    requirements: List[str]\\n\\n\\nllm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\\nllm_with_tool = llm.bind_tools([PromptInstructions])\\n\\ngather_chain = gather_prompt | llm_with_tool\\n\\n\\nasync def information_gather(state, writer: StreamWriter):\\n    \"\"\"사용자와 상호작용하며 필요한 요구 사항을 수집하는 노드\"\"\"\\n    print(\"--- [Information_gather] ---\")\\n    messages = state[\"messages\"]\\n    print(messages)\\n    response = []\\n    tool_calls = None\\n\\n    first = True\\n    async for chunk in gather_chain.astream({\"messages\": messages}):\\n        if chunk.tool_call_chunks:\\n            if first:\\n                tool_calls = chunk\\n                first = False\\n            else:\\n                tool_calls += chunk\\n        elif chunk.content:\\n            response.append(chunk.content)\\n            writer(chunk.content)\\n        else:\\n            continue\\n\\n    if response:\\n        response = AIMessage(content=\"\".join(response))\\n        return {\"messages\": [response]}\\n    else:\\n        return {\"messages\": [tool_calls]}\\n\\n\\n# * generate prompt\\nwith open(\"./prompts/generate_prompt.yaml\", \"r\") as f:\\n    generate_template = yaml.safe_load(f)\\n\\n\\ndef get_prompt_messages(messages: list):\\n    tool_call = None\\n    other_messages = []\\n\\n    for msg in messages:\\n        if isinstance(msg, AIMessage) and msg.tool_calls:\\n            tool_call = msg.tool_calls[0][\"args\"]\\n        elif isinstance(msg, ToolMessage):\\n            continue\\n        # tool call 이후의 메세지 추가 (추가적인 대화 내용)\\n        elif tool_call is not None:\\n            other_messages.append(msg)\\n    return [\\n        SystemMessage(\\n            content=generate_template[\"template\"].format(requirements=tool_call)\\n        )\\n    ] + other_messages\\n\\n\\nasync def generate_prompt(state, writer: StreamWriter):\\n    print(\"--- [GENERATE PROMPT] ---\")\\n    messages = get_prompt_messages(state[\"messages\"])\\n    print(messages)\\n    response = []\\n    async for chunk in llm.astream(messages):\\n        response.append(chunk.content)\\n        # print(chunk)\\n        # print(\"---\")\\n        writer(chunk.content)\\n\\n    return {\"messages\": [AIMessage(content=\"\".join(response))]}\\n\\n\\n# * add_tool_message 노드 정의\\ndef add_tool_message(state):\\n    return {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"prompt 생성 완료!\",\\n                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\\n            )\\n        ]\\n    }\\n\\n\\nif __name__ == \"__main__\":\\n    print(\\n        gather_chain.invoke(\\n            {\"messages\": [(\"user\", \"안녕하세요, 프롬프트 만들어주세요.\")]}\\n        ).content\\n    )\\n\\n```', name='get_file_contents', id='b65f3143-ec3c-4f69-a337-a87ce4efb9a7', tool_call_id='call_tJScz9GBDwO0oncOO5v98YK6'), ToolMessage(content='role: \"system\"\\ntemplate: |\\n  You are an expert prompt engineer. Your task is to generate a well-structured, detailed, and effective prompt based on a given set of requirements.\\n\\n  ## Instructions:\\n  - Read and understand the provided requirements carefully.\\n  - Ensure the generated prompt is clear, specific, and optimized for an LLM response.\\n  - Use precise language and structure the prompt logically.\\n  - If the requirements are ambiguous, make reasonable assumptions and clarify them in the prompt.\\n  - If necessary, break the prompt into multiple steps to guide the model\\'s response effectively.\\n\\n  ## [IMPORTANT]\\n  - variables must be expressed in {{variable_name}} format with curly braces.\\n\\n\\n  ## Requirements:\\n  {requirements}\\n\\n  Now, generate the best possible prompt based on the above requirements.\\ninput_variables: [\"requirements\"]', name='get_file_contents', id='0ddc2cff-c6a6-4bfd-8746-ee7a81821478', tool_call_id='call_qesMqoGJSqMqA3KEf7D6YdOq'), ToolMessage(content='role: \"system\"\\ntemplate: |\\n  Your job is to get information from a user about what type of prompt template they want to create.\\n\\n  You should get the following information from them:\\n\\n  - What the objective of the prompt is\\n  - What variables will be passed into the prompt template\\n  - Any constraints for what the output should NOT do\\n  - Any requirements that the output MUST adhere to\\n\\n  [IMPORTANT!]\\n  - You must communicate in Korean. However, the prompt generation must be in English.\\n  - If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\\n  - After you are able to discern all the information, call the relevant tool.\\ninput_variables: []', name='get_file_contents', id='cccc94a2-6d8d-4d77-ba13-9d440d885f0f', tool_call_id='call_7KpU68z4YeZ4uRoKleOh8cAG'), AIMessage(content='프로젝트의 주요 파일 내용을 확인했습니다. 각 파일의 역할과 프로젝트의 전체 로직을 다음과 같이 설명할 수 있습니다.\\n\\n1. **`app.py`**: \\n   - Streamlit을 사용하여 웹 애플리케이션을 구성합니다. \\n   - 사용자가 입력한 메시지를 기반으로 AI 응답을 생성하고, 이를 실시간으로 스트리밍하여 사용자에게 보여줍니다.\\n   - 세션 상태를 관리하여 이전 메시지를 저장하고, 새로운 메시지를 추가합니다.\\n\\n2. **`main.py`**:\\n   - FastAPI를 사용하여 서버를 구성합니다.\\n   - `/chat/stream` 엔드포인트를 통해 실시간 스트리밍 응답을 제공합니다.\\n   - `graph` 모듈을 사용하여 입력 메시지를 처리하고, 결과를 스트리밍합니다.\\n\\n3. **`graph/graph.py`**:\\n   - `StateGraph`를 사용하여 상태 그래프를 정의합니다.\\n   - 노드와 엣지를 추가하여 정보 수집, 프롬프트 생성, 도구 메시지 추가 등의 작업을 수행합니다.\\n   - 그래프를 컴파일하여 실행 가능한 형태로 만듭니다.\\n\\n4. **`graph/edges.py`**:\\n   - 상태에 따라 다음 노드를 결정하는 로직을 정의합니다.\\n   - AI 메시지에 도구 호출이 포함되어 있는지 여부에 따라 다음 상태를 결정합니다.\\n\\n5. **`graph/nodes.py`**:\\n   - 정보 수집, 프롬프트 생성, 도구 메시지 추가 등의 노드를 정의합니다.\\n   - 각 노드는 비동기적으로 실행되며, 메시지를 처리하고 결과를 반환합니다.\\n\\n6. **`prompts/generate_prompt.yaml` 및 `prompts/gather.yaml`**:\\n   - 프롬프트 생성을 위한 템플릿을 정의합니다.\\n   - `generate_prompt.yaml`은 프롬프트 생성 시 사용할 템플릿을, `gather.yaml`은 사용자로부터 정보를 수집할 때 사용할 템플릿을 정의합니다.\\n\\n**전체 로직**:\\n- 사용자가 웹 애플리케이션을 통해 메시지를 입력하면, 이 메시지는 FastAPI 서버로 전송됩니다.\\n- 서버는 `graph` 모듈을 통해 메시지를 처리하고, 상태 그래프에 따라 정보 수집, 프롬프트 생성 등의 작업을 수행합니다.\\n- 최종적으로 생성된 프롬프트는 사용자에게 스트리밍 방식으로 전달됩니다.\\n\\n이러한 구조를 통해 프로젝트는 사용자와의 상호작용을 기반으로 효과적인 프롬프트를 생성하고, 이를 실시간으로 제공하는 기능을 구현하고 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 570, 'prompt_tokens': 4181, 'total_tokens': 4751, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1024}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3c0e617e-a293-427b-be79-a7d541d27d35-0', usage_metadata={'input_tokens': 4181, 'output_tokens': 570, 'total_tokens': 4751, 'input_token_details': {'audio': 0, 'cache_read': 1024}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='해당 파일 간의 연결 관계를 좀 더 명확히 시각화해주실 수 있을까요?', additional_kwargs={}, response_metadata={}, id='4e20454b-fed8-4753-a731-8a363c823d5c'), AIMessage(content='프로젝트의 파일 간 연결 관계를 명확히 시각화하기 위해 각 파일이 어떤 역할을 하고, 다른 파일과 어떻게 상호작용하는지를 설명하겠습니다. 이를 통해 프로젝트의 구조를 더 잘 이해할 수 있습니다.\\n\\n1. **`app.py`**:\\n   - **역할**: 사용자 인터페이스를 제공하는 웹 애플리케이션.\\n   - **연결**: 사용자가 입력한 메시지를 `main.py`의 FastAPI 서버로 전송하여 처리 결과를 받습니다.\\n\\n2. **`main.py`**:\\n   - **역할**: FastAPI 서버로, 클라이언트로부터 요청을 받아 처리하고 응답을 스트리밍합니다.\\n   - **연결**: `graph/graph.py`의 `graph` 객체를 사용하여 메시지를 처리합니다.\\n\\n3. **`graph/graph.py`**:\\n   - **역할**: 상태 그래프를 정의하고 컴파일하여 메시지 처리 로직을 구현합니다.\\n   - **연결**: `graph/nodes.py`와 `graph/edges.py`의 노드 및 엣지를 사용하여 그래프를 구성합니다.\\n\\n4. **`graph/edges.py`**:\\n   - **역할**: 상태 전환 로직을 정의하여 다음 노드를 결정합니다.\\n   - **연결**: `graph/graph.py`에서 상태 전환 시 사용됩니다.\\n\\n5. **`graph/nodes.py`**:\\n   - **역할**: 정보 수집, 프롬프트 생성 등의 노드를 정의합니다.\\n   - **연결**: `graph/graph.py`에서 각 노드를 추가하여 그래프를 구성합니다.\\n\\n6. **`prompts/generate_prompt.yaml` 및 `prompts/gather.yaml`**:\\n   - **역할**: 프롬프트 생성을 위한 템플릿을 제공합니다.\\n   - **연결**: `graph/nodes.py`에서 프롬프트 생성 시 사용됩니다.\\n\\n**시각화**:\\n- `app.py`는 사용자와의 인터페이스를 담당하며, `main.py`로 메시지를 전송합니다.\\n- `main.py`는 `graph/graph.py`의 `graph` 객체를 호출하여 메시지를 처리합니다.\\n- `graph/graph.py`는 `graph/nodes.py`와 `graph/edges.py`의 정의를 사용하여 상태 그래프를 구성합니다.\\n- `graph/nodes.py`는 `prompts/generate_prompt.yaml` 및 `prompts/gather.yaml`을 사용하여 프롬프트를 생성합니다.\\n\\n이러한 연결 관계를 통해 프로젝트는 사용자 입력을 처리하고, 이를 기반으로 프롬프트를 생성하여 사용자에게 제공하는 기능을 수행합니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 586, 'prompt_tokens': 4780, 'total_tokens': 5366, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4736}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3ac76c31-f0f2-475d-a6ec-7422c0c0173c-0', usage_metadata={'input_tokens': 4780, 'output_tokens': 586, 'total_tokens': 5366, 'input_token_details': {'audio': 0, 'cache_read': 4736}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='main.py 파일을 자세하게 설명해주세요.', additional_kwargs={}, response_metadata={}, id='96c02f15-b951-4772-a357-0e596f36ac60')]\n",
      "\n",
      "[ messages ]\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "`main.py` 파일은 FastAPI를 사용하여 서버를 구성하고, 클라이언트로부터의 요청을 처리하여 실시간으로 응답을 스트리밍하는 역할을 합니다. 이 파일의 주요 구성 요소와 동작을 자세히 설명하겠습니다.\n",
      "\n",
      "### 주요 구성 요소\n",
      "\n",
      "1. **FastAPI 앱 초기화**:\n",
      "   ```python\n",
      "   app = FastAPI()\n",
      "   ```\n",
      "   - FastAPI 애플리케이션 인스턴스를 생성합니다. 이 인스턴스를 통해 API 엔드포인트를 정의하고 요청을 처리합니다.\n",
      "\n",
      "2. **엔드포인트 정의**:\n",
      "   ```python\n",
      "   @app.get(\"/chat/stream\")\n",
      "   async def stream(query: str, thread_id: str = \"1\"):\n",
      "   ```\n",
      "   - `/chat/stream` 경로에 대한 GET 요청을 처리하는 엔드포인트를 정의합니다.\n",
      "   - `query`와 `thread_id`라는 두 개의 쿼리 매개변수를 받습니다. `query`는 사용자가 입력한 메시지이고, `thread_id`는 세션을 식별하는 데 사용됩니다.\n",
      "\n",
      "3. **이벤트 스트림 생성**:\n",
      "   ```python\n",
      "   async def event_stream():\n",
      "       try:\n",
      "           async for chunk in graph.astream(\n",
      "               input={\"messages\": [HumanMessage(content=query)]},\n",
      "               config=config,\n",
      "               stream_mode=[\"custom\"],\n",
      "           ):\n",
      "               yield f\"data: {json.dumps({'text': chunk[1]})}\\n\\n\"\n",
      "       except Exception as e:\n",
      "           print(f\"========== {e}\")\n",
      "           yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n",
      "   ```\n",
      "   - `event_stream` 함수는 비동기 제너레이터로, `graph.astream`을 호출하여 메시지를 처리하고, 결과를 스트리밍합니다.\n",
      "   - 각 청크(chunk)는 JSON 형식으로 변환되어 클라이언트로 전송됩니다.\n",
      "   - 예외가 발생하면 오류 메시지를 스트리밍합니다.\n",
      "\n",
      "4. **StreamingResponse 반환**:\n",
      "   ```python\n",
      "   return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n",
      "   ```\n",
      "   - `StreamingResponse` 객체를 반환하여 클라이언트에게 실시간 스트리밍 응답을 제공합니다.\n",
      "   - `event_stream` 함수에서 생성된 데이터를 스트리밍합니다.\n",
      "\n",
      "### 동작 설명\n",
      "\n",
      "- 클라이언트가 `/chat/stream` 엔드포인트에 GET 요청을 보내면, `stream` 함수가 호출됩니다.\n",
      "- `stream` 함수는 `event_stream` 비동기 제너레이터를 통해 `graph` 객체의 `astream` 메서드를 호출하여 메시지를 처리합니다.\n",
      "- 처리된 결과는 JSON 형식으로 변환되어 `StreamingResponse`를 통해 클라이언트에게 실시간으로 전송됩니다.\n",
      "- 이 과정에서 발생할 수 있는 예외는 잡아서 오류 메시지로 클라이언트에게 전달됩니다.\n",
      "\n",
      "`main.py` 파일은 FastAPI를 활용하여 비동기적으로 메시지를 처리하고, 실시간 스트리밍 응답을 제공하는 핵심적인 역할을 수행합니다.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "config = {\"recursion_limit\": 10, \"configurable\": {\"thread_id\": \"abc\"}}\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"main.py 파일을 자세하게 설명해주세요.\")]}\n",
    "\n",
    "# stream_graph(graph, inputs, config)\n",
    "\n",
    "for event in graph.stream(\n",
    "    input=inputs,\n",
    "    config=config,\n",
    "    stream_mode=\"values\",  # 각 노드의 출력 상태\n",
    "    # interrupt_before=[\"tool_node\"],  # tool_node로 가기 전에 멈추기!\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n[ {key} ]\\n\")\n",
    "\n",
    "        print(value[-1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='전체 프로젝트 로직에 대해 설명해주세요.', additional_kwargs={}, response_metadata={}, id='cfb41c39-66b3-4cc9-a8f8-6f371885db01'),\n",
       "  AIMessage(content='프로젝트의 전체 로직을 설명하기 위해서는 프로젝트의 디렉토리 구조와 각 파일의 내용을 확인해야 합니다. 우선 프로젝트의 디렉토리 구조를 확인하겠습니다. 어느 디렉토리의 구조를 확인할까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 221, 'total_tokens': 276, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'stop', 'logprobs': None}, id='run-80a001bf-9a0a-4e0e-a9c4-4d74083dc412-0', usage_metadata={'input_tokens': 221, 'output_tokens': 55, 'total_tokens': 276, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='네, /home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt 입니다.', additional_kwargs={}, response_metadata={}, id='458df4a5-318f-48a0-ba29-d0064c03195d'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sB771upNNtW89bjiYexVYtzA', 'function': {'arguments': '{\"root_dir\":\"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt\"}', 'name': 'get_directory_structure'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 300, 'total_tokens': 330, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-139b86d7-b757-48b3-b071-76e4bb2e4f9e-0', tool_calls=[{'name': 'get_directory_structure', 'args': {'root_dir': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt'}, 'id': 'call_sB771upNNtW89bjiYexVYtzA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 300, 'output_tokens': 30, 'total_tokens': 330, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='<directory_structure>\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n</directory_structure>', name='get_directory_structure', id='3bf059b5-b7d6-45fb-a0b0-1217323a80dd', tool_call_id='call_sB771upNNtW89bjiYexVYtzA'),\n",
       "  AIMessage(content='디렉토리 구조는 다음과 같습니다:\\n\\n- `meta_prompt/`\\n  - `.env`\\n  - `requirements.txt`\\n  - `README.md`\\n  - `graph.png`\\n  - `app.py`\\n  - `.env.example`\\n  - `main.py`\\n  - `Dockerfile`\\n  - `graph/`\\n    - `graph.py`\\n    - `edges.py`\\n    - `nodes.py`\\n  - `prompts/`\\n    - `generate_prompt.yaml`\\n    - `gather.yaml`\\n\\n이제 각 파일의 내용을 확인하여 전체 로직을 설명할 수 있습니다. 어떤 파일부터 확인할까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 457, 'total_tokens': 591, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a761218-ee79-47cf-b4b5-67f9d4c6e40a-0', usage_metadata={'input_tokens': 457, 'output_tokens': 134, 'total_tokens': 591, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='알아서 판단해주세요', additional_kwargs={}, response_metadata={}, id='359207a8-2724-4bfd-8f0b-87dca730b84a'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_qZkmhAUhkkeYm6k1MM79j8Tq', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/README.md\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_LGIiR9bxGOK70GnzxbXyn1V3', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_JeuTdSGssrNziVUW55K0jfYw', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_FqftYvEoNy15oSWig0M16iJB', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'id': 'call_W2mi7iQpuriuePYBQhvPAr7m', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml\"}', 'name': 'get_file_contents'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 601, 'total_tokens': 781, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-6a6cbde4-9149-4cf1-91ea-6f45f91e005a-0', tool_calls=[{'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/README.md'}, 'id': 'call_qZkmhAUhkkeYm6k1MM79j8Tq', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py'}, 'id': 'call_LGIiR9bxGOK70GnzxbXyn1V3', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py'}, 'id': 'call_JeuTdSGssrNziVUW55K0jfYw', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py'}, 'id': 'call_FqftYvEoNy15oSWig0M16iJB', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/prompts/generate_prompt.yaml'}, 'id': 'call_W2mi7iQpuriuePYBQhvPAr7m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 601, 'output_tokens': 180, 'total_tokens': 781, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='# Meta-Prompt Generator\\n\\nLangGraph를 활용한 대화형 메타 프롬프트 생성 시스템입니다. 이 시스템은 사용자와의 대화를 통해 최적화된 프롬프트를 생성하고, 생성된 프롬프트를 실시간으로 확인할 수 있는 웹 인터페이스를 제공합니다.\\n\\n## 주요 기능\\n\\n- 🤖 대화형 메타 프롬프트 생성\\n- 🌊 실시간 스트리밍 응답\\n- 📝 프롬프트 최적화 및 검증\\n- 🎯 맞춤형 프롬프트 템플릿 생성\\n- 🔄 LangGraph 기반의 프롬프트 처리 파이프라인\\n- 🌐 Streamlit 기반의 웹 인터페이스\\n\\n## 시스템 아키텍처\\n\\n![System Architecture](graph.png)\\n\\n## 기술 스택\\n\\n- 🐍 Python 3.11+\\n- ⚡ FastAPI\\n- 🔄 LangGraph\\n- 🦜 LangChain\\n- 🎈 Streamlit\\n- 🐳 Docker\\n\\n## 시작하기\\n\\n### 로컬 환경에서 실행\\n\\n1. 환경 설정\\n```bash\\n# 저장소 클론\\ngit clone https://github.com/yourusername/meta-prompt-generator.git\\ncd meta-prompt-generator\\n\\n# 가상환경 생성\\npython -m venv venv\\nsource venv/bin/activate  # Windows: venv\\\\Scripts\\\\activate\\n\\n# 의존성 설치\\npip install -r requirements.txt\\n\\n# 환경 변수 설정\\ncp .env.example .env\\n# .env 파일을 편집하여 필요한 API 키 설정\\n```\\n\\n2. 서버 실행\\n```bash\\n# FastAPI 서버 실행\\nuvicorn main:app --reload --port 8000\\n\\n# 새로운 터미널에서 Streamlit 앱 실행\\nstreamlit run app.py\\n```\\n\\n### Docker로 실행\\n\\n```bash\\n# 이미지 빌드\\ndocker build -t meta-prompt-generator .\\n\\n# 컨테이너 실행\\ndocker run -p 8000:8000 -p 8501:8501 meta-prompt-generator\\n```\\n\\n## 사용 방법\\n\\n1. 웹 브라우저에서 `http://localhost:8501` 접속\\n2. 채팅 인터페이스에서 프롬프트 생성 요구사항 입력\\n3. 실시간으로 생성되는 프롬프트 확인 및 수정\\n\\n## API 엔드포인트\\n\\n- GET `/chat/stream`: 스트리밍 채팅 API\\n  - Parameters:\\n    - `query`: 사용자 입력 텍스트\\n    - `thread_id`: 채팅 스레드 ID (선택사항)\\n\\n## 프로젝트 구조\\n\\n```\\n.\\n├── app.py              # Streamlit 웹 인터페이스\\n├── main.py            # FastAPI 서버\\n├── graph/             # LangGraph 관련 코드\\n│   ├── graph.py      # 메인 그래프 정의\\n│   ├── nodes.py      # 그래프 노드 정의\\n│   └── edges.py      # 그래프 엣지 정의\\n├── prompts/          # YAML 형식의 프롬프트 템플릿\\n├── requirements.txt  # 프로젝트 의존성\\n├── Dockerfile       # 컨테이너화 설정\\n└── README.md       # 프로젝트 문서\\n```\\n\\n## 환경 변수\\n\\n`.env` 파일에 다음 환경 변수를 설정해야 합니다. 기본값은 gpt-4o 입니다.\\n\\n```\\nOPENAI_API_KEY=your_openai_api_key\\nMODEL_NAME=gpt-4o\\nTEMPERATURE=0.7\\n```\\n', name='get_file_contents', id='dc7663a2-9820-46a5-83c3-183c7c212d7e', tool_call_id='call_qZkmhAUhkkeYm6k1MM79j8Tq'),\n",
       "  ToolMessage(content='import streamlit as st\\nimport requests\\nimport json\\nimport time\\nimport uuid\\nfrom typing import Optional\\n\\nst.set_page_config(page_title=\"Meta Prompt Generator\", page_icon=\"🤖\", layout=\"wide\")\\n\\nst.title(\"Meta Prompt Generator 🤖\")\\nst.markdown(\"LLM과 대화하며 프롬프트를 생성해보세요.\")\\n\\n# 세션 상태 초기화\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\n# thread_id 초기화 (세션별 고유 ID)\\nif \"thread_id\" not in st.session_state:\\n    st.session_state.thread_id = str(uuid.uuid4())\\n\\n# 이전 메시지 표시\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# 사용자 입력\\nif prompt := st.chat_input(\"메시지를 입력하세요\"):\\n    # 사용자 메시지 표시\\n    with st.chat_message(\"human\"):\\n        st.markdown(prompt)\\n\\n    # 메시지 저장\\n    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\\n\\n    # AI 응답 생성\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n\\n        try:\\n            # SSE 스트림 연결\\n            with requests.get(\\n                \"http://localhost:8000/chat/stream\",\\n                params={\"query\": prompt, \"thread_id\": st.session_state.thread_id},\\n                headers={\"Accept\": \"text/event-stream\"},\\n                stream=True,\\n            ) as response:\\n                response.raise_for_status()\\n\\n                for line in response.iter_lines(decode_unicode=True):\\n                    if line.strip():\\n                        if line.startswith(\"data: \"):\\n                            try:\\n                                # JSON 파싱\\n                                data = json.loads(line.split(\"data: \")[-1])\\n\\n                                if \"text\" in data:\\n                                    full_response += data[\"text\"]\\n                                    message_placeholder.markdown(full_response + \"▌\")\\n                                elif \"error\" in data:\\n                                    st.error(f\"Error: {data[\\'error\\']}\")\\n                                    break\\n\\n                            except json.JSONDecodeError as e:\\n                                st.error(f\"JSON 파싱 에러: {e}\")\\n                                continue\\n\\n                            time.sleep(0.01)\\n\\n            # 최종 응답 표시\\n            message_placeholder.markdown(full_response)\\n\\n            # AI 응답 저장\\n            st.session_state.messages.append(\\n                {\"role\": \"assistant\", \"content\": full_response}\\n            )\\n\\n        except requests.exceptions.RequestException as e:\\n            st.error(f\"네트워크 에러: {str(e)}\")\\n        except Exception as e:\\n            st.error(f\"예상치 못한 에러: {str(e)}\")\\n\\n# 디버깅용 세션 정보 표시 (필요시 주석 해제)\\n# st.sidebar.write(\"Session Thread ID:\", st.session_state.thread_id)\\n', name='get_file_contents', id='fe9d1657-022f-4461-ad60-4ec8610b1289', tool_call_id='call_LGIiR9bxGOK70GnzxbXyn1V3'),\n",
       "  ToolMessage(content='from fastapi import FastAPI\\nfrom fastapi.responses import StreamingResponse\\nfrom langchain_core.messages import HumanMessage\\nfrom graph.graph import graph\\nimport json\\n\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/chat/stream\")\\nasync def stream(query: str, thread_id: str = \"1\"):\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n    async def event_stream():\\n        try:\\n            async for chunk in graph.astream(\\n                input={\"messages\": [HumanMessage(content=query)]},\\n                config=config,\\n                stream_mode=[\"custom\"],\\n            ):\\n                yield f\"data: {json.dumps({\\'text\\': chunk[1]})}\\\\n\\\\n\"\\n        except Exception as e:\\n            print(f\"========== {e}\")\\n            yield f\"data: {json.dumps({\\'error\\': str(e)})}\\\\n\\\\n\"\\n\\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\\n', name='get_file_contents', id='51ec0f04-6200-40d8-88dc-d82d45a8e56d', tool_call_id='call_JeuTdSGssrNziVUW55K0jfYw'),\n",
       "  ToolMessage(content='from langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated, TypedDict\\n\\nfrom graph.nodes import information_gather, generate_prompt, add_tool_message\\nfrom graph.edges import get_state\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\n\\nmemory = MemorySaver()\\n\\nflow = StateGraph(State)\\nflow.add_node(\"information_gather\", information_gather)\\nflow.add_node(\"generate_prompt\", generate_prompt)\\nflow.add_node(\"add_tool_message\", add_tool_message)\\n\\nflow.add_edge(START, \"information_gather\")\\nflow.add_conditional_edges(\\n    \"information_gather\",\\n    get_state,\\n    {\\n        \"add_tool_message\": \"add_tool_message\",\\n        END: END,\\n    },\\n)\\nflow.add_edge(\"add_tool_message\", \"generate_prompt\")\\nflow.add_edge(\"generate_prompt\", END)\\n\\n\\ngraph = flow.compile(checkpointer=memory)\\n\\n\\ngraph.get_graph().draw_mermaid_png(output_file_path=\"./graph.png\")\\n', name='get_file_contents', id='2f2df6ec-e028-436f-bc56-ac52a1842a34', tool_call_id='call_FqftYvEoNy15oSWig0M16iJB'),\n",
       "  ToolMessage(content='role: \"system\"\\ntemplate: |\\n  You are an expert prompt engineer. Your task is to generate a well-structured, detailed, and effective prompt based on a given set of requirements.\\n\\n  ## Instructions:\\n  - Read and understand the provided requirements carefully.\\n  - Ensure the generated prompt is clear, specific, and optimized for an LLM response.\\n  - Use precise language and structure the prompt logically.\\n  - If the requirements are ambiguous, make reasonable assumptions and clarify them in the prompt.\\n  - If necessary, break the prompt into multiple steps to guide the model\\'s response effectively.\\n\\n  ## [IMPORTANT]\\n  - variables must be expressed in {{variable_name}} format with curly braces.\\n\\n\\n  ## Requirements:\\n  {requirements}\\n\\n  Now, generate the best possible prompt based on the above requirements.\\ninput_variables: [\"requirements\"]', name='get_file_contents', id='01e5dd1f-7f3b-4bf6-94ba-44862792d8b6', tool_call_id='call_W2mi7iQpuriuePYBQhvPAr7m'),\n",
       "  AIMessage(content='프로젝트의 전체 로직은 다음과 같습니다:\\n\\n1. **프로젝트 개요**:\\n   - 이 프로젝트는 LangGraph를 활용하여 대화형 메타 프롬프트를 생성하는 시스템입니다. 사용자는 웹 인터페이스를 통해 프롬프트를 생성하고 실시간으로 확인할 수 있습니다.\\n\\n2. **주요 구성 요소**:\\n   - `app.py`: Streamlit을 사용하여 웹 인터페이스를 제공합니다. 사용자가 입력한 메시지를 FastAPI 서버로 전송하고, 서버로부터의 응답을 실시간으로 표시합니다.\\n   - `main.py`: FastAPI 서버로, 사용자의 쿼리를 받아 LangGraph를 통해 처리하고 스트리밍 응답을 제공합니다.\\n   - `graph/graph.py`: LangGraph의 상태 그래프를 정의하고, 노드와 엣지를 추가하여 프롬프트 생성 흐름을 관리합니다.\\n   - `prompts/generate_prompt.yaml`: 프롬프트 생성에 필요한 템플릿을 정의합니다. 주어진 요구사항에 따라 최적화된 프롬프트를 생성합니다.\\n\\n3. **기술 스택**:\\n   - Python 3.11+, FastAPI, LangGraph, LangChain, Streamlit, Docker 등을 사용합니다.\\n\\n4. **실행 방법**:\\n   - 로컬 환경에서 Python 가상 환경을 설정하고 필요한 패키지를 설치한 후, FastAPI 서버와 Streamlit 앱을 실행합니다.\\n   - Docker를 사용하여 컨테이너화된 환경에서도 실행할 수 있습니다.\\n\\n5. **사용 방법**:\\n   - 웹 브라우저에서 `http://localhost:8501`에 접속하여 프롬프트 생성 요구사항을 입력하고, 실시간으로 생성된 프롬프트를 확인할 수 있습니다.\\n\\n이 프로젝트는 대화형으로 프롬프트를 생성하고, 이를 실시간으로 확인할 수 있는 기능을 제공합니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 408, 'prompt_tokens': 4584, 'total_tokens': 4992, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a0e0e19-b79a-4a8f-a7f8-2d1669908564-0', usage_metadata={'input_tokens': 4584, 'output_tokens': 408, 'total_tokens': 4992, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
