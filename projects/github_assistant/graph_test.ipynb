{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로젝트의 디렉토리 구조는 다음과 같습니다:\n",
      "\n",
      "```\n",
      "meta_prompt/\n",
      "    .env\n",
      "    requirements.txt\n",
      "    README.md\n",
      "    graph.png\n",
      "    app.py\n",
      "    .env.example\n",
      "    main.py\n",
      "    Dockerfile\n",
      "    __pycache__/\n",
      "        main.cpython-311.pyc\n",
      "    graph/\n",
      "        graph.py\n",
      "        edges.py\n",
      "        nodes.py\n",
      "        __pycache__/\n",
      "            graph.cpython-311.pyc\n",
      "            nodes.cpython-311.pyc\n",
      "            edges.cpython-311.pyc\n",
      "    prompts/\n",
      "        generate_prompt.yaml\n",
      "        gather.yaml\n",
      "```\n",
      "\n",
      "이제 각 파일의 역할과 프로젝트의 전체적인 구조를 이해하기 위해 주요 파일들의 내용을 살펴보겠습니다. `main.py`, `app.py`, 그리고 `graph/` 디렉토리 내의 파일들(`graph.py`, `edges.py`, `nodes.py`)의 내용을 확인해 보겠습니다.프로젝트의 주요 파일들의 내용을 통해 전체적인 구조와 흐름을 이해할 수 있습니다.\n",
      "\n",
      "1. **`app.py`**:\n",
      "   - FastAPI를 사용하여 웹 서버를 설정합니다.\n",
      "   - `/chat/stream` 엔드포인트를 통해 클라이언트로부터 쿼리를 받아 비동기적으로 처리하고, 결과를 스트리밍 방식으로 반환합니다.\n",
      "   - `graph.astream`을 호출하여 그래프의 노드를 순차적으로 실행합니다.\n",
      "\n",
      "2. **`main.py`**:\n",
      "   - Streamlit을 사용하여 웹 애플리케이션을 구축합니다.\n",
      "   - 사용자로부터 입력을 받아 FastAPI 서버의 `/chat/stream` 엔드포인트에 요청을 보내고, 응답을 받아 화면에 표시합니다.\n",
      "   - 세션 상태를 관리하여 이전 메시지를 저장하고 표시합니다.\n",
      "\n",
      "3. **`graph/graph.py`**:\n",
      "   - `StateGraph`를 사용하여 그래프의 노드와 엣지를 정의합니다.\n",
      "   - `information_gather`, `generate_prompt`, `add_tool_message` 노드를 추가하고, 각 노드 간의 연결을 설정합니다.\n",
      "   - 그래프를 컴파일하여 실행 가능한 형태로 만듭니다.\n",
      "\n",
      "4. **`graph/edges.py`**:\n",
      "   - 상태에 따라 다음 노드를 결정하는 `get_state` 함수를 정의합니다.\n",
      "   - AI 메시지에 도구 호출이 포함되어 있는지 확인하여 다음 상태를 결정합니다.\n",
      "\n",
      "5. **`graph/nodes.py`**:\n",
      "   - `information_gather`, `generate_prompt`, `add_tool_message` 노드를 정의합니다.\n",
      "   - 각 노드는 특정 작업을 수행하며, 비동기적으로 실행됩니다.\n",
      "   - `information_gather`는 사용자와 상호작용하여 필요한 요구 사항을 수집합니다.\n",
      "   - `generate_prompt`는 수집된 정보를 바탕으로 프롬프트를 생성합니다.\n",
      "   - `add_tool_message`는 도구 호출 메시지를 추가합니다.\n",
      "\n",
      "### 전체적인 흐름\n",
      "1. **사용자 입력**: `main.py`에서 사용자로부터 입력을 받습니다.\n",
      "2. **서버 요청**: 입력된 쿼리를 `app.py`의 FastAPI 서버로 전송합니다.\n",
      "3. **그래프 실행**: `graph.py`에서 정의된 그래프가 실행되며, 각 노드가 순차적으로 처리됩니다.\n",
      "4. **결과 반환**: 처리된 결과를 스트리밍 방식으로 클라이언트에 반환합니다.\n",
      "5. **결과 표시**: `main.py`에서 반환된 결과를 화면에 표시합니다.\n",
      "\n",
      "이러한 구조를 통해 사용자 입력을 기반으로 프롬프트를 생성하고, 이를 실시간으로 사용자에게 제공하는 시스템을 구축하고 있습니다."
     ]
    }
   ],
   "source": [
    "from graph.graph import create_summary_agent\n",
    "\n",
    "\n",
    "graph = create_summary_agent()\n",
    "config = {\"recursion_limit\": 10, \"configurable\": {\"thread_id\": \"abc\"}}\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\n",
    "            \"user\",\n",
    "            \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt 입니다\",\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# for chunk, metadata in graph.stream(\n",
    "#     input=inputs,\n",
    "#     config=config,\n",
    "#     stream_mode=\"messages\",  # 각 노드의 출력 상태\n",
    "# ):\n",
    "#     if chunk.content:\n",
    "#         print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    input=inputs,\n",
    "    config=config,\n",
    "    stream_mode=\"custom\",  # 각 노드의 출력 상태\n",
    "):\n",
    "    if chunk:\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt 입니다', additional_kwargs={}, response_metadata={}, id='87810605-4c50-48c4-b97e-70456693a8f6'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_FIkmWGqyM5sZPYJeY976GuRQ', 'function': {'arguments': '{\"root_dir\":\"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt\"}', 'name': 'get_directory_structure'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e'}, id='run-28297478-74e0-49b5-9882-05370a823939', tool_calls=[{'name': 'get_directory_structure', 'args': {'root_dir': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt'}, 'id': 'call_FIkmWGqyM5sZPYJeY976GuRQ', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='<directory_structure>\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n</directory_structure>', name='get_directory_structure', id='e476f246-ca17-4f0f-8161-2acce4d45af1', tool_call_id='call_FIkmWGqyM5sZPYJeY976GuRQ'),\n",
       "  AIMessage(content='프로젝트의 디렉토리 구조는 다음과 같습니다:\\n\\n```\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n```\\n\\n이제 각 파일의 역할과 프로젝트의 전체적인 구조를 이해하기 위해 주요 파일들의 내용을 살펴보겠습니다. `main.py`, `app.py`, 그리고 `graph/` 디렉토리 내의 파일들(`graph.py`, `edges.py`, `nodes.py`)의 내용을 확인해 보겠습니다.', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_nM0A1AgltJPPnAPHMbehfw7T', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 1, 'id': 'call_HALotc363LKM2kY13sgiaeqv', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 2, 'id': 'call_G54RCv9bAarZ74tjWduY6kDJ', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 3, 'id': 'call_lCn9rok45enKeUp4XWCPYBjF', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 4, 'id': 'call_3yFrju0eDYVFgQzlNfghYLa6', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e'}, id='run-d47a8a81-d831-4a29-a95e-a8e9c863475f', tool_calls=[{'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py'}, 'id': 'call_nM0A1AgltJPPnAPHMbehfw7T', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py'}, 'id': 'call_HALotc363LKM2kY13sgiaeqv', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py'}, 'id': 'call_G54RCv9bAarZ74tjWduY6kDJ', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py'}, 'id': 'call_lCn9rok45enKeUp4XWCPYBjF', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py'}, 'id': 'call_3yFrju0eDYVFgQzlNfghYLa6', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='```python\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import StreamingResponse\\nfrom langchain_core.messages import HumanMessage\\nfrom graph.graph import graph\\nimport json\\n\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/chat/stream\")\\nasync def stream(query: str, thread_id: str = \"1\"):\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n    async def event_stream():\\n        try:\\n            async for chunk in graph.astream(\\n                input={\"messages\": [HumanMessage(content=query)]},\\n                config=config,\\n                stream_mode=[\"custom\"],\\n            ):\\n                yield f\"data: {json.dumps({\\'text\\': chunk[1]})}\\\\n\\\\n\"\\n        except Exception as e:\\n            print(f\"========== {e}\")\\n            yield f\"data: {json.dumps({\\'error\\': str(e)})}\\\\n\\\\n\"\\n\\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\\n\\n```', name='get_file_contents', id='0536cafa-8f62-4321-8b1c-778a4b23ce3d', tool_call_id='call_nM0A1AgltJPPnAPHMbehfw7T'),\n",
       "  ToolMessage(content='```python\\nimport streamlit as st\\nimport requests\\nimport json\\nimport time\\nimport uuid\\nfrom typing import Optional\\n\\nst.set_page_config(page_title=\"Meta Prompt Generator\", page_icon=\"🤖\", layout=\"wide\")\\n\\nst.title(\"Meta Prompt Generator 🤖\")\\nst.markdown(\"LLM과 대화하며 프롬프트를 생성해보세요.\")\\n\\n# 세션 상태 초기화\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\n# thread_id 초기화 (세션별 고유 ID)\\nif \"thread_id\" not in st.session_state:\\n    st.session_state.thread_id = str(uuid.uuid4())\\n\\n# 이전 메시지 표시\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# 사용자 입력\\nif prompt := st.chat_input(\"메시지를 입력하세요\"):\\n    # 사용자 메시지 표시\\n    with st.chat_message(\"human\"):\\n        st.markdown(prompt)\\n\\n    # 메시지 저장\\n    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\\n\\n    # AI 응답 생성\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n\\n        try:\\n            # SSE 스트림 연결\\n            with requests.get(\\n                \"http://localhost:8000/chat/stream\",\\n                params={\"query\": prompt, \"thread_id\": st.session_state.thread_id},\\n                headers={\"Accept\": \"text/event-stream\"},\\n                stream=True,\\n            ) as response:\\n                response.raise_for_status()\\n\\n                for line in response.iter_lines(decode_unicode=True):\\n                    if line.strip():\\n                        if line.startswith(\"data: \"):\\n                            try:\\n                                # JSON 파싱\\n                                data = json.loads(line.split(\"data: \")[-1])\\n\\n                                if \"text\" in data:\\n                                    full_response += data[\"text\"]\\n                                    message_placeholder.markdown(full_response + \"▌\")\\n                                elif \"error\" in data:\\n                                    st.error(f\"Error: {data[\\'error\\']}\")\\n                                    break\\n\\n                            except json.JSONDecodeError as e:\\n                                st.error(f\"JSON 파싱 에러: {e}\")\\n                                continue\\n\\n                            time.sleep(0.01)\\n\\n            # 최종 응답 표시\\n            message_placeholder.markdown(full_response)\\n\\n            # AI 응답 저장\\n            st.session_state.messages.append(\\n                {\"role\": \"assistant\", \"content\": full_response}\\n            )\\n\\n        except requests.exceptions.RequestException as e:\\n            st.error(f\"네트워크 에러: {str(e)}\")\\n        except Exception as e:\\n            st.error(f\"예상치 못한 에러: {str(e)}\")\\n\\n# 디버깅용 세션 정보 표시 (필요시 주석 해제)\\n# st.sidebar.write(\"Session Thread ID:\", st.session_state.thread_id)\\n\\n```', name='get_file_contents', id='d70fd63c-14bf-44b3-bb08-d21c6641f7b5', tool_call_id='call_HALotc363LKM2kY13sgiaeqv'),\n",
       "  ToolMessage(content='```python\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated, TypedDict\\n\\nfrom graph.nodes import information_gather, generate_prompt, add_tool_message\\nfrom graph.edges import get_state\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\n\\nmemory = MemorySaver()\\n\\nflow = StateGraph(State)\\nflow.add_node(\"information_gather\", information_gather)\\nflow.add_node(\"generate_prompt\", generate_prompt)\\nflow.add_node(\"add_tool_message\", add_tool_message)\\n\\nflow.add_edge(START, \"information_gather\")\\nflow.add_conditional_edges(\\n    \"information_gather\",\\n    get_state,\\n    {\\n        \"add_tool_message\": \"add_tool_message\",\\n        END: END,\\n    },\\n)\\nflow.add_edge(\"add_tool_message\", \"generate_prompt\")\\nflow.add_edge(\"generate_prompt\", END)\\n\\n\\ngraph = flow.compile(checkpointer=memory)\\n\\n\\ngraph.get_graph().draw_mermaid_png(output_file_path=\"./graph.png\")\\n\\n```', name='get_file_contents', id='38e023b7-3b37-4805-bebc-04aefbc169fb', tool_call_id='call_G54RCv9bAarZ74tjWduY6kDJ'),\n",
       "  ToolMessage(content='```python\\nfrom langgraph.graph import END\\n\\nfrom langchain_core.messages import AIMessage\\n\\n\\ndef get_state(state):\\n    messages = state[\"messages\"]\\n    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\\n        return \"add_tool_message\"\\n    return END\\n\\n```', name='get_file_contents', id='edf4e7a5-fe4c-4d7b-a11b-1a554d31d449', tool_call_id='call_lCn9rok45enKeUp4XWCPYBjF'),\n",
       "  ToolMessage(content='```python\\nimport yaml\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_core.messages import SystemMessage, AIMessage, ToolMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.types import StreamWriter\\nfrom pydantic import BaseModel\\nfrom typing import List\\n\\nload_dotenv()\\nMODEL_NAME = os.getenv(\"MODEL_NAME\")\\nTEMPERATURE = os.getenv(\"TEMPERATURE\")\\n\\n# * information gather\\n\\nwith open(\"./prompts/gather.yaml\", \"r\") as f:\\n    gather_template = yaml.safe_load(f)\\n\\ngather_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (gather_template[\"role\"], gather_template[\"template\"]),\\n        MessagesPlaceholder(variable_name=\"messages\"),\\n    ]\\n)\\n\\n\\nclass PromptInstructions(BaseModel):\\n    \"\"\"Instructions on how to prompt the LLM.\"\"\"\\n\\n    objective: str\\n    variables: List[str]\\n    constraints: List[str]\\n    requirements: List[str]\\n\\n\\nllm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\\nllm_with_tool = llm.bind_tools([PromptInstructions])\\n\\ngather_chain = gather_prompt | llm_with_tool\\n\\n\\nasync def information_gather(state, writer: StreamWriter):\\n    \"\"\"사용자와 상호작용하며 필요한 요구 사항을 수집하는 노드\"\"\"\\n    print(\"--- [Information_gather] ---\")\\n    messages = state[\"messages\"]\\n    print(messages)\\n    response = []\\n    tool_calls = None\\n\\n    first = True\\n    async for chunk in gather_chain.astream({\"messages\": messages}):\\n        if chunk.tool_call_chunks:\\n            if first:\\n                tool_calls = chunk\\n                first = False\\n            else:\\n                tool_calls += chunk\\n        elif chunk.content:\\n            response.append(chunk.content)\\n            writer(chunk.content)\\n        else:\\n            continue\\n\\n    if response:\\n        response = AIMessage(content=\"\".join(response))\\n        return {\"messages\": [response]}\\n    else:\\n        return {\"messages\": [tool_calls]}\\n\\n\\n# * generate prompt\\nwith open(\"./prompts/generate_prompt.yaml\", \"r\") as f:\\n    generate_template = yaml.safe_load(f)\\n\\n\\ndef get_prompt_messages(messages: list):\\n    tool_call = None\\n    other_messages = []\\n\\n    for msg in messages:\\n        if isinstance(msg, AIMessage) and msg.tool_calls:\\n            tool_call = msg.tool_calls[0][\"args\"]\\n        elif isinstance(msg, ToolMessage):\\n            continue\\n        # tool call 이후의 메세지 추가 (추가적인 대화 내용)\\n        elif tool_call is not None:\\n            other_messages.append(msg)\\n    return [\\n        SystemMessage(\\n            content=generate_template[\"template\"].format(requirements=tool_call)\\n        )\\n    ] + other_messages\\n\\n\\nasync def generate_prompt(state, writer: StreamWriter):\\n    print(\"--- [GENERATE PROMPT] ---\")\\n    messages = get_prompt_messages(state[\"messages\"])\\n    print(messages)\\n    response = []\\n    async for chunk in llm.astream(messages):\\n        response.append(chunk.content)\\n        # print(chunk)\\n        # print(\"---\")\\n        writer(chunk.content)\\n\\n    return {\"messages\": [AIMessage(content=\"\".join(response))]}\\n\\n\\n# * add_tool_message 노드 정의\\ndef add_tool_message(state):\\n    return {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"prompt 생성 완료!\",\\n                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\\n            )\\n        ]\\n    }\\n\\n\\nif __name__ == \"__main__\":\\n    print(\\n        gather_chain.invoke(\\n            {\"messages\": [(\"user\", \"안녕하세요, 프롬프트 만들어주세요.\")]}\\n        ).content\\n    )\\n\\n```', name='get_file_contents', id='4dee4a0c-ea83-4d61-989c-5a20988396da', tool_call_id='call_3yFrju0eDYVFgQzlNfghYLa6'),\n",
       "  AIMessage(content='프로젝트의 주요 파일들의 내용을 통해 전체적인 구조와 흐름을 이해할 수 있습니다.\\n\\n### 주요 파일 및 역할\\n\\n1. **`app.py`**:\\n   - FastAPI를 사용하여 웹 서버를 설정합니다.\\n   - `/chat/stream` 엔드포인트를 통해 클라이언트로부터 쿼리를 받아 비동기적으로 처리하고, 결과를 스트리밍 방식으로 반환합니다.\\n   - `graph` 모듈의 `astream` 메서드를 사용하여 메시지를 처리합니다.\\n\\n2. **`main.py`**:\\n   - Streamlit을 사용하여 웹 애플리케이션을 구축합니다.\\n   - 사용자로부터 입력을 받아 FastAPI 서버에 요청을 보내고, 서버로부터 받은 응답을 화면에 표시합니다.\\n   - 세션 상태를 관리하여 이전 메시지를 저장하고 표시합니다.\\n\\n3. **`graph/graph.py`**:\\n   - `StateGraph`를 사용하여 상태 그래프를 정의합니다.\\n   - 노드와 엣지를 추가하여 그래프의 흐름을 설정합니다.\\n   - `information_gather`, `generate_prompt`, `add_tool_message` 노드를 포함하며, 각 노드 간의 전이 조건을 정의합니다.\\n\\n4. **`graph/edges.py`**:\\n   - 상태에 따라 다음 노드를 결정하는 `get_state` 함수를 정의합니다.\\n   - 메시지의 마지막 요소가 AIMessage이고 도구 호출이 있는 경우 `add_tool_message`로 전이합니다.\\n\\n5. **`graph/nodes.py`**:\\n   - `information_gather`, `generate_prompt`, `add_tool_message` 함수를 정의하여 각 노드의 동작을 구현합니다.\\n   - `information_gather`: 사용자와 상호작용하여 필요한 요구 사항을 수집합니다.\\n   - `generate_prompt`: 수집된 정보를 바탕으로 프롬프트를 생성합니다.\\n   - `add_tool_message`: 프롬프트 생성 완료 메시지를 추가합니다.\\n\\n### 전체적인 흐름\\n\\n1. **사용자 입력**: `main.py`에서 사용자가 메시지를 입력합니다.\\n2. **서버 요청**: 입력된 메시지는 FastAPI 서버(`app.py`)로 전송됩니다.\\n3. **그래프 처리**: 서버는 `graph` 모듈을 사용하여 메시지를 처리합니다.\\n   - `information_gather` 노드에서 정보를 수집하고,\\n   - `generate_prompt` 노드에서 프롬프트를 생성하며,\\n   - `add_tool_message` 노드에서 완료 메시지를 추가합니다.\\n4. **응답 반환**: 처리된 결과는 스트리밍 방식으로 클라이언트에 반환됩니다.\\n5. **결과 표시**: `main.py`에서 받은 응답을 화면에 표시합니다.\\n\\n이러한 구조를 통해 사용자와의 대화형 프롬프트 생성 시스템을 구현하고 있습니다.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e'}, id='run-9d3e62b3-a693-4fce-bb67-033a404a1dcf')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = graph.get_state(config)\n",
    "state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
