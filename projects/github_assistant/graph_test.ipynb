{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "```\n",
      "meta_prompt/\n",
      "    .env\n",
      "    requirements.txt\n",
      "    README.md\n",
      "    graph.png\n",
      "    app.py\n",
      "    .env.example\n",
      "    main.py\n",
      "    Dockerfile\n",
      "    __pycache__/\n",
      "        main.cpython-311.pyc\n",
      "    graph/\n",
      "        graph.py\n",
      "        edges.py\n",
      "        nodes.py\n",
      "        __pycache__/\n",
      "            graph.cpython-311.pyc\n",
      "            nodes.cpython-311.pyc\n",
      "            edges.cpython-311.pyc\n",
      "    prompts/\n",
      "        generate_prompt.yaml\n",
      "        gather.yaml\n",
      "```\n",
      "\n",
      "ì´ì œ ê° íŒŒì¼ì˜ ì—­í• ê³¼ í”„ë¡œì íŠ¸ì˜ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì£¼ìš” íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. `main.py`, `app.py`, ê·¸ë¦¬ê³  `graph/` ë””ë ‰í† ë¦¬ ë‚´ì˜ íŒŒì¼ë“¤(`graph.py`, `edges.py`, `nodes.py`)ì˜ ë‚´ìš©ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.í”„ë¡œì íŠ¸ì˜ ì£¼ìš” íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ í†µí•´ ì „ì²´ì ì¸ êµ¬ì¡°ì™€ íë¦„ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **`app.py`**:\n",
      "   - FastAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ì„œë²„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "   - `/chat/stream` ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ì¿¼ë¦¬ë¥¼ ë°›ì•„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "   - `graph.astream`ì„ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜í”„ì˜ ë…¸ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **`main.py`**:\n",
      "   - Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
      "   - ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ì•„ FastAPI ì„œë²„ì˜ `/chat/stream` ì—”ë“œí¬ì¸íŠ¸ì— ìš”ì²­ì„ ë³´ë‚´ê³ , ì‘ë‹µì„ ë°›ì•„ í™”ë©´ì— í‘œì‹œí•©ë‹ˆë‹¤.\n",
      "   - ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ì—¬ ì´ì „ ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê³  í‘œì‹œí•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **`graph/graph.py`**:\n",
      "   - `StateGraph`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ì˜ ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
      "   - `information_gather`, `generate_prompt`, `add_tool_message` ë…¸ë“œë¥¼ ì¶”ê°€í•˜ê³ , ê° ë…¸ë“œ ê°„ì˜ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "   - ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
      "\n",
      "4. **`graph/edges.py`**:\n",
      "   - ìƒíƒœì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•˜ëŠ” `get_state` í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
      "   - AI ë©”ì‹œì§€ì— ë„êµ¬ í˜¸ì¶œì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ë‹¤ìŒ ìƒíƒœë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **`graph/nodes.py`**:\n",
      "   - `information_gather`, `generate_prompt`, `add_tool_message` ë…¸ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
      "   - ê° ë…¸ë“œëŠ” íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë©°, ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
      "   - `information_gather`ëŠ” ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ í•„ìš”í•œ ìš”êµ¬ ì‚¬í•­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
      "   - `generate_prompt`ëŠ” ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "   - `add_tool_message`ëŠ” ë„êµ¬ í˜¸ì¶œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ì „ì²´ì ì¸ íë¦„\n",
      "1. **ì‚¬ìš©ì ì…ë ¥**: `main.py`ì—ì„œ ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.\n",
      "2. **ì„œë²„ ìš”ì²­**: ì…ë ¥ëœ ì¿¼ë¦¬ë¥¼ `app.py`ì˜ FastAPI ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.\n",
      "3. **ê·¸ë˜í”„ ì‹¤í–‰**: `graph.py`ì—ì„œ ì •ì˜ëœ ê·¸ë˜í”„ê°€ ì‹¤í–‰ë˜ë©°, ê° ë…¸ë“œê°€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n",
      "4. **ê²°ê³¼ ë°˜í™˜**: ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜í•©ë‹ˆë‹¤.\n",
      "5. **ê²°ê³¼ í‘œì‹œ**: `main.py`ì—ì„œ ë°˜í™˜ëœ ê²°ê³¼ë¥¼ í™”ë©´ì— í‘œì‹œí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ í†µí•´ ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "from graph.graph import create_summary_agent\n",
    "\n",
    "\n",
    "graph = create_summary_agent()\n",
    "config = {\"recursion_limit\": 10, \"configurable\": {\"thread_id\": \"abc\"}}\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\n",
    "            \"user\",\n",
    "            \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt ì…ë‹ˆë‹¤\",\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# for chunk, metadata in graph.stream(\n",
    "#     input=inputs,\n",
    "#     config=config,\n",
    "#     stream_mode=\"messages\",  # ê° ë…¸ë“œì˜ ì¶œë ¥ ìƒíƒœ\n",
    "# ):\n",
    "#     if chunk.content:\n",
    "#         print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    input=inputs,\n",
    "    config=config,\n",
    "    stream_mode=\"custom\",  # ê° ë…¸ë“œì˜ ì¶œë ¥ ìƒíƒœ\n",
    "):\n",
    "    if chunk:\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt ì…ë‹ˆë‹¤', additional_kwargs={}, response_metadata={}, id='87810605-4c50-48c4-b97e-70456693a8f6'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_FIkmWGqyM5sZPYJeY976GuRQ', 'function': {'arguments': '{\"root_dir\":\"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt\"}', 'name': 'get_directory_structure'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e'}, id='run-28297478-74e0-49b5-9882-05370a823939', tool_calls=[{'name': 'get_directory_structure', 'args': {'root_dir': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt'}, 'id': 'call_FIkmWGqyM5sZPYJeY976GuRQ', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='<directory_structure>\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n</directory_structure>', name='get_directory_structure', id='e476f246-ca17-4f0f-8161-2acce4d45af1', tool_call_id='call_FIkmWGqyM5sZPYJeY976GuRQ'),\n",
       "  AIMessage(content='í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n```\\nmeta_prompt/\\n    .env\\n    requirements.txt\\n    README.md\\n    graph.png\\n    app.py\\n    .env.example\\n    main.py\\n    Dockerfile\\n    __pycache__/\\n        main.cpython-311.pyc\\n    graph/\\n        graph.py\\n        edges.py\\n        nodes.py\\n        __pycache__/\\n            graph.cpython-311.pyc\\n            nodes.cpython-311.pyc\\n            edges.cpython-311.pyc\\n    prompts/\\n        generate_prompt.yaml\\n        gather.yaml\\n```\\n\\nì´ì œ ê° íŒŒì¼ì˜ ì—­í• ê³¼ í”„ë¡œì íŠ¸ì˜ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì£¼ìš” íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. `main.py`, `app.py`, ê·¸ë¦¬ê³  `graph/` ë””ë ‰í† ë¦¬ ë‚´ì˜ íŒŒì¼ë“¤(`graph.py`, `edges.py`, `nodes.py`)ì˜ ë‚´ìš©ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_nM0A1AgltJPPnAPHMbehfw7T', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 1, 'id': 'call_HALotc363LKM2kY13sgiaeqv', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 2, 'id': 'call_G54RCv9bAarZ74tjWduY6kDJ', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 3, 'id': 'call_lCn9rok45enKeUp4XWCPYBjF', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}, {'index': 4, 'id': 'call_3yFrju0eDYVFgQzlNfghYLa6', 'function': {'arguments': '{\"file_path\": \"/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py\"}', 'name': 'get_file_contents'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e'}, id='run-d47a8a81-d831-4a29-a95e-a8e9c863475f', tool_calls=[{'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/main.py'}, 'id': 'call_nM0A1AgltJPPnAPHMbehfw7T', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/app.py'}, 'id': 'call_HALotc363LKM2kY13sgiaeqv', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/graph.py'}, 'id': 'call_G54RCv9bAarZ74tjWduY6kDJ', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/edges.py'}, 'id': 'call_lCn9rok45enKeUp4XWCPYBjF', 'type': 'tool_call'}, {'name': 'get_file_contents', 'args': {'file_path': '/home/sean/PythonProject/langgraph/langgraph/projects/meta_prompt/graph/nodes.py'}, 'id': 'call_3yFrju0eDYVFgQzlNfghYLa6', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='```python\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import StreamingResponse\\nfrom langchain_core.messages import HumanMessage\\nfrom graph.graph import graph\\nimport json\\n\\n\\napp = FastAPI()\\n\\n\\n@app.get(\"/chat/stream\")\\nasync def stream(query: str, thread_id: str = \"1\"):\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n    async def event_stream():\\n        try:\\n            async for chunk in graph.astream(\\n                input={\"messages\": [HumanMessage(content=query)]},\\n                config=config,\\n                stream_mode=[\"custom\"],\\n            ):\\n                yield f\"data: {json.dumps({\\'text\\': chunk[1]})}\\\\n\\\\n\"\\n        except Exception as e:\\n            print(f\"========== {e}\")\\n            yield f\"data: {json.dumps({\\'error\\': str(e)})}\\\\n\\\\n\"\\n\\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\\n\\n```', name='get_file_contents', id='0536cafa-8f62-4321-8b1c-778a4b23ce3d', tool_call_id='call_nM0A1AgltJPPnAPHMbehfw7T'),\n",
       "  ToolMessage(content='```python\\nimport streamlit as st\\nimport requests\\nimport json\\nimport time\\nimport uuid\\nfrom typing import Optional\\n\\nst.set_page_config(page_title=\"Meta Prompt Generator\", page_icon=\"ğŸ¤–\", layout=\"wide\")\\n\\nst.title(\"Meta Prompt Generator ğŸ¤–\")\\nst.markdown(\"LLMê³¼ ëŒ€í™”í•˜ë©° í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ë³´ì„¸ìš”.\")\\n\\n# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\n# thread_id ì´ˆê¸°í™” (ì„¸ì…˜ë³„ ê³ ìœ  ID)\\nif \"thread_id\" not in st.session_state:\\n    st.session_state.thread_id = str(uuid.uuid4())\\n\\n# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\n# ì‚¬ìš©ì ì…ë ¥\\nif prompt := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”\"):\\n    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\\n    with st.chat_message(\"human\"):\\n        st.markdown(prompt)\\n\\n    # ë©”ì‹œì§€ ì €ì¥\\n    st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\\n\\n    # AI ì‘ë‹µ ìƒì„±\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n\\n        try:\\n            # SSE ìŠ¤íŠ¸ë¦¼ ì—°ê²°\\n            with requests.get(\\n                \"http://localhost:8000/chat/stream\",\\n                params={\"query\": prompt, \"thread_id\": st.session_state.thread_id},\\n                headers={\"Accept\": \"text/event-stream\"},\\n                stream=True,\\n            ) as response:\\n                response.raise_for_status()\\n\\n                for line in response.iter_lines(decode_unicode=True):\\n                    if line.strip():\\n                        if line.startswith(\"data: \"):\\n                            try:\\n                                # JSON íŒŒì‹±\\n                                data = json.loads(line.split(\"data: \")[-1])\\n\\n                                if \"text\" in data:\\n                                    full_response += data[\"text\"]\\n                                    message_placeholder.markdown(full_response + \"â–Œ\")\\n                                elif \"error\" in data:\\n                                    st.error(f\"Error: {data[\\'error\\']}\")\\n                                    break\\n\\n                            except json.JSONDecodeError as e:\\n                                st.error(f\"JSON íŒŒì‹± ì—ëŸ¬: {e}\")\\n                                continue\\n\\n                            time.sleep(0.01)\\n\\n            # ìµœì¢… ì‘ë‹µ í‘œì‹œ\\n            message_placeholder.markdown(full_response)\\n\\n            # AI ì‘ë‹µ ì €ì¥\\n            st.session_state.messages.append(\\n                {\"role\": \"assistant\", \"content\": full_response}\\n            )\\n\\n        except requests.exceptions.RequestException as e:\\n            st.error(f\"ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {str(e)}\")\\n        except Exception as e:\\n            st.error(f\"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}\")\\n\\n# ë””ë²„ê¹…ìš© ì„¸ì…˜ ì •ë³´ í‘œì‹œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)\\n# st.sidebar.write(\"Session Thread ID:\", st.session_state.thread_id)\\n\\n```', name='get_file_contents', id='d70fd63c-14bf-44b3-bb08-d21c6641f7b5', tool_call_id='call_HALotc363LKM2kY13sgiaeqv'),\n",
       "  ToolMessage(content='```python\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated, TypedDict\\n\\nfrom graph.nodes import information_gather, generate_prompt, add_tool_message\\nfrom graph.edges import get_state\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\n\\nmemory = MemorySaver()\\n\\nflow = StateGraph(State)\\nflow.add_node(\"information_gather\", information_gather)\\nflow.add_node(\"generate_prompt\", generate_prompt)\\nflow.add_node(\"add_tool_message\", add_tool_message)\\n\\nflow.add_edge(START, \"information_gather\")\\nflow.add_conditional_edges(\\n    \"information_gather\",\\n    get_state,\\n    {\\n        \"add_tool_message\": \"add_tool_message\",\\n        END: END,\\n    },\\n)\\nflow.add_edge(\"add_tool_message\", \"generate_prompt\")\\nflow.add_edge(\"generate_prompt\", END)\\n\\n\\ngraph = flow.compile(checkpointer=memory)\\n\\n\\ngraph.get_graph().draw_mermaid_png(output_file_path=\"./graph.png\")\\n\\n```', name='get_file_contents', id='38e023b7-3b37-4805-bebc-04aefbc169fb', tool_call_id='call_G54RCv9bAarZ74tjWduY6kDJ'),\n",
       "  ToolMessage(content='```python\\nfrom langgraph.graph import END\\n\\nfrom langchain_core.messages import AIMessage\\n\\n\\ndef get_state(state):\\n    messages = state[\"messages\"]\\n    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\\n        return \"add_tool_message\"\\n    return END\\n\\n```', name='get_file_contents', id='edf4e7a5-fe4c-4d7b-a11b-1a554d31d449', tool_call_id='call_lCn9rok45enKeUp4XWCPYBjF'),\n",
       "  ToolMessage(content='```python\\nimport yaml\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_core.messages import SystemMessage, AIMessage, ToolMessage\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.types import StreamWriter\\nfrom pydantic import BaseModel\\nfrom typing import List\\n\\nload_dotenv()\\nMODEL_NAME = os.getenv(\"MODEL_NAME\")\\nTEMPERATURE = os.getenv(\"TEMPERATURE\")\\n\\n# * information gather\\n\\nwith open(\"./prompts/gather.yaml\", \"r\") as f:\\n    gather_template = yaml.safe_load(f)\\n\\ngather_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (gather_template[\"role\"], gather_template[\"template\"]),\\n        MessagesPlaceholder(variable_name=\"messages\"),\\n    ]\\n)\\n\\n\\nclass PromptInstructions(BaseModel):\\n    \"\"\"Instructions on how to prompt the LLM.\"\"\"\\n\\n    objective: str\\n    variables: List[str]\\n    constraints: List[str]\\n    requirements: List[str]\\n\\n\\nllm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\\nllm_with_tool = llm.bind_tools([PromptInstructions])\\n\\ngather_chain = gather_prompt | llm_with_tool\\n\\n\\nasync def information_gather(state, writer: StreamWriter):\\n    \"\"\"ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ë©° í•„ìš”í•œ ìš”êµ¬ ì‚¬í•­ì„ ìˆ˜ì§‘í•˜ëŠ” ë…¸ë“œ\"\"\"\\n    print(\"--- [Information_gather] ---\")\\n    messages = state[\"messages\"]\\n    print(messages)\\n    response = []\\n    tool_calls = None\\n\\n    first = True\\n    async for chunk in gather_chain.astream({\"messages\": messages}):\\n        if chunk.tool_call_chunks:\\n            if first:\\n                tool_calls = chunk\\n                first = False\\n            else:\\n                tool_calls += chunk\\n        elif chunk.content:\\n            response.append(chunk.content)\\n            writer(chunk.content)\\n        else:\\n            continue\\n\\n    if response:\\n        response = AIMessage(content=\"\".join(response))\\n        return {\"messages\": [response]}\\n    else:\\n        return {\"messages\": [tool_calls]}\\n\\n\\n# * generate prompt\\nwith open(\"./prompts/generate_prompt.yaml\", \"r\") as f:\\n    generate_template = yaml.safe_load(f)\\n\\n\\ndef get_prompt_messages(messages: list):\\n    tool_call = None\\n    other_messages = []\\n\\n    for msg in messages:\\n        if isinstance(msg, AIMessage) and msg.tool_calls:\\n            tool_call = msg.tool_calls[0][\"args\"]\\n        elif isinstance(msg, ToolMessage):\\n            continue\\n        # tool call ì´í›„ì˜ ë©”ì„¸ì§€ ì¶”ê°€ (ì¶”ê°€ì ì¸ ëŒ€í™” ë‚´ìš©)\\n        elif tool_call is not None:\\n            other_messages.append(msg)\\n    return [\\n        SystemMessage(\\n            content=generate_template[\"template\"].format(requirements=tool_call)\\n        )\\n    ] + other_messages\\n\\n\\nasync def generate_prompt(state, writer: StreamWriter):\\n    print(\"--- [GENERATE PROMPT] ---\")\\n    messages = get_prompt_messages(state[\"messages\"])\\n    print(messages)\\n    response = []\\n    async for chunk in llm.astream(messages):\\n        response.append(chunk.content)\\n        # print(chunk)\\n        # print(\"---\")\\n        writer(chunk.content)\\n\\n    return {\"messages\": [AIMessage(content=\"\".join(response))]}\\n\\n\\n# * add_tool_message ë…¸ë“œ ì •ì˜\\ndef add_tool_message(state):\\n    return {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"prompt ìƒì„± ì™„ë£Œ!\",\\n                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\\n            )\\n        ]\\n    }\\n\\n\\nif __name__ == \"__main__\":\\n    print(\\n        gather_chain.invoke(\\n            {\"messages\": [(\"user\", \"ì•ˆë…•í•˜ì„¸ìš”, í”„ë¡¬í”„íŠ¸ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\")]}\\n        ).content\\n    )\\n\\n```', name='get_file_contents', id='4dee4a0c-ea83-4d61-989c-5a20988396da', tool_call_id='call_3yFrju0eDYVFgQzlNfghYLa6'),\n",
       "  AIMessage(content='í”„ë¡œì íŠ¸ì˜ ì£¼ìš” íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ í†µí•´ ì „ì²´ì ì¸ êµ¬ì¡°ì™€ íë¦„ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n### ì£¼ìš” íŒŒì¼ ë° ì—­í• \\n\\n1. **`app.py`**:\\n   - FastAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ì„œë²„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\\n   - `/chat/stream` ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ì¿¼ë¦¬ë¥¼ ë°›ì•„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\\n   - `graph` ëª¨ë“ˆì˜ `astream` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\\n\\n2. **`main.py`**:\\n   - Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\\n   - ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ì•„ FastAPI ì„œë²„ì— ìš”ì²­ì„ ë³´ë‚´ê³ , ì„œë²„ë¡œë¶€í„° ë°›ì€ ì‘ë‹µì„ í™”ë©´ì— í‘œì‹œí•©ë‹ˆë‹¤.\\n   - ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ì—¬ ì´ì „ ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê³  í‘œì‹œí•©ë‹ˆë‹¤.\\n\\n3. **`graph/graph.py`**:\\n   - `StateGraph`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\\n   - ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì¶”ê°€í•˜ì—¬ ê·¸ë˜í”„ì˜ íë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.\\n   - `information_gather`, `generate_prompt`, `add_tool_message` ë…¸ë“œë¥¼ í¬í•¨í•˜ë©°, ê° ë…¸ë“œ ê°„ì˜ ì „ì´ ì¡°ê±´ì„ ì •ì˜í•©ë‹ˆë‹¤.\\n\\n4. **`graph/edges.py`**:\\n   - ìƒíƒœì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•˜ëŠ” `get_state` í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\\n   - ë©”ì‹œì§€ì˜ ë§ˆì§€ë§‰ ìš”ì†Œê°€ AIMessageì´ê³  ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° `add_tool_message`ë¡œ ì „ì´í•©ë‹ˆë‹¤.\\n\\n5. **`graph/nodes.py`**:\\n   - `information_gather`, `generate_prompt`, `add_tool_message` í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬ ê° ë…¸ë“œì˜ ë™ì‘ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\\n   - `information_gather`: ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ì—¬ í•„ìš”í•œ ìš”êµ¬ ì‚¬í•­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\\n   - `generate_prompt`: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\\n   - `add_tool_message`: í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\\n\\n### ì „ì²´ì ì¸ íë¦„\\n\\n1. **ì‚¬ìš©ì ì…ë ¥**: `main.py`ì—ì„œ ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.\\n2. **ì„œë²„ ìš”ì²­**: ì…ë ¥ëœ ë©”ì‹œì§€ëŠ” FastAPI ì„œë²„(`app.py`)ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.\\n3. **ê·¸ë˜í”„ ì²˜ë¦¬**: ì„œë²„ëŠ” `graph` ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\\n   - `information_gather` ë…¸ë“œì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ ,\\n   - `generate_prompt` ë…¸ë“œì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ë©°,\\n   - `add_tool_message` ë…¸ë“œì—ì„œ ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\\n4. **ì‘ë‹µ ë°˜í™˜**: ì²˜ë¦¬ëœ ê²°ê³¼ëŠ” ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜ë©ë‹ˆë‹¤.\\n5. **ê²°ê³¼ í‘œì‹œ**: `main.py`ì—ì„œ ë°›ì€ ì‘ë‹µì„ í™”ë©´ì— í‘œì‹œí•©ë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ í†µí•´ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff092ab25e'}, id='run-9d3e62b3-a693-4fce-bb67-033a404a1dcf')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = graph.get_state(config)\n",
    "state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
